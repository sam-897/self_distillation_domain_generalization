{"cells":[{"cell_type":"markdown","metadata":{},"source":["SE BLOCK for SENET, basicblock for RESNET"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T21:35:05.298913Z","iopub.status.busy":"2024-04-25T21:35:05.298163Z","iopub.status.idle":"2024-04-25T21:35:05.317684Z","shell.execute_reply":"2024-04-25T21:35:05.316879Z","shell.execute_reply.started":"2024-04-25T21:35:05.298879Z"},"trusted":true},"outputs":[],"source":["## ResNet Blocks and SE Blocks\n","\n","import math\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, bias=False)\n","                     \n","class SELayer(nn.Module):\n","    def __init__(self, channel, reduction=16):\n","        super(SELayer, self).__init__()\n","        # print('se reduction: ', reduction)\n","        # print(channel // reduction)\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1) # F_squeeze \n","        self.fc = nn.Sequential(\n","            nn.Linear(channel, channel // reduction, bias=False),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(channel // reduction, channel, bias=False),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):   # x: B*C*D*T\n","        b, c, _, _ = x.size()\n","        y = self.avg_pool(x).view(b, c)\n","        y = self.fc(y).view(b, c, 1, 1)\n","        return x * y.expand_as(x)\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","class SEBasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=16):\n","        super(SEBasicBlock, self).__init__()\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes, 1)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.se = SELayer(planes, reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.se(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n"]},{"cell_type":"markdown","metadata":{},"source":["ECANET"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T18:56:42.039003Z","iopub.status.busy":"2024-04-25T18:56:42.038144Z","iopub.status.idle":"2024-04-25T18:56:42.046244Z","shell.execute_reply":"2024-04-25T18:56:42.045384Z","shell.execute_reply.started":"2024-04-25T18:56:42.038969Z"},"trusted":true},"outputs":[],"source":["# import torch\n","# from torch import nn\n","from torch.nn.parameter import Parameter\n","\n","class eca_layer(nn.Module):\n","    def __init__(self, channel, k_size=3):\n","        super(eca_layer, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # feature descriptor on the global spatial information\n","        y = self.avg_pool(x)\n","\n","        # Two different branches of ECA module\n","        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n","\n","        # Multi-scale information fusion\n","        y = self.sigmoid(y)\n","\n","        return x * y.expand_as(x)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T20:20:17.993238Z","iopub.status.busy":"2024-04-25T20:20:17.992900Z","iopub.status.idle":"2024-04-25T20:20:17.998265Z","shell.execute_reply":"2024-04-25T20:20:17.997174Z","shell.execute_reply.started":"2024-04-25T20:20:17.993214Z"},"trusted":true},"outputs":[],"source":["def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, bias=False)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T20:01:31.249350Z","iopub.status.busy":"2024-04-24T20:01:31.248537Z","iopub.status.idle":"2024-04-24T20:01:31.258642Z","shell.execute_reply":"2024-04-24T20:01:31.257513Z","shell.execute_reply.started":"2024-04-24T20:01:31.249315Z"},"trusted":true},"outputs":[],"source":["class ECABasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, k_size=3):\n","        super(ECABasicBlock, self).__init__()\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes, 1)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.eca = eca_layer(planes, k_size)\n","        self.downsample = downsample\n","        self.stride = stride\n","#         self.dropout= nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","#         out=self.dropout(out)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","#         \n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.eca(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n"]},{"cell_type":"markdown","metadata":{},"source":["MODELS : ECANET,SENET,RESNET"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T21:35:17.252519Z","iopub.status.busy":"2024-04-25T21:35:17.252151Z","iopub.status.idle":"2024-04-25T21:35:17.273764Z","shell.execute_reply":"2024-04-25T21:35:17.272770Z","shell.execute_reply.started":"2024-04-25T21:35:17.252489Z"},"trusted":true},"outputs":[],"source":["import math\n","# import torch\n","# import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.model_zoo as model_zoo\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, layers, num_classes, KaimingInit=False):\n","\n","        self.inplanes = 16\n","\n","        super(ResNet, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm2d(16)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        self.layer1 = self._make_layer(block, 16, layers[0])\n","        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 128, layers[3], stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.middle_fc1 = nn.Linear(16* block.expansion, 2)\n","        self.middle_fc2=nn.Linear(32* block.expansion, 2)\n","        self.middle_fc3=nn.Linear(64* block.expansion, 2)\n","        self.classifier = nn.Linear(128 * block.expansion, 2)\n","#         self.dropout= nn.Dropout(0.2)\n","\n","        if KaimingInit == True:\n","            print('Using Kaiming Initialization.')\n","            for m in self.modules():\n","                if isinstance(m, nn.Conv2d):\n","                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion)\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        #print(x.size())\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","#         x=self.dropout(x)\n","        x = self.maxpool(x)\n","        #print(x.size())\n","\n","        x = self.layer1(x)\n","        # return middle output1\n","        middle_output1=self.avgpool(x).view(x.size()[0], -1)\n","        middle_output1 = self.middle_fc1(middle_output1)\n","        #print(x.size())\n","        x = self.layer2(x)\n","        #return middle output2\n","        middle_output2=self.avgpool(x).view(x.size()[0], -1)\n","        middle_output2 = self.middle_fc2(middle_output2)\n","        #print(x.size())\n","        x = self.layer3(x)\n","        #return middle output3\n","        middle_output3=self.avgpool(x).view(x.size()[0], -1)\n","        middle_output3 = self.middle_fc3(middle_output3)\n","        #print(x.size())\n","        x = self.layer4(x)\n","        #print(x.size())\n","        x = self.avgpool(x).view(x.size()[0], -1)\n","        #print(x.shape)\n","        out = self.classifier(x)\n","        #print(out.shape)\n","        return middle_output1,middle_output2,middle_output3,out,x\n","\n","def eca_resnet18():\n","    model = ResNet(ECABasicBlock, [2, 2, 2, 2], 2)\n","    return model\n","\n","\n","def eca_resnet34():\n","    model = ResNet(ECABasicBlock, [3, 4, 6, 3], 2)\n","    return model\n","\n","def se_resnet18():\n","    model = ResNet(SEBasicBlock,, [2, 2, 2, 2], 2)\n","    return model\n","\n","def se_resnet34():\n","    model = ResNet(SEBasicBlock,, [3, 4, 6, 3], 2)\n","    return model\n","\n","def resnet18():\n","    model=ResNet(BasicBlock,[2,2,2,2],2)\n","    return model\n","\n","def resnet34():\n","    model = ResNet(BasicBlock, [3, 4, 6, 3], 2)\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:32:10.292563Z","iopub.status.busy":"2024-04-14T16:32:10.292196Z","iopub.status.idle":"2024-04-14T16:32:24.839773Z","shell.execute_reply":"2024-04-14T16:32:24.838720Z","shell.execute_reply.started":"2024-04-14T16:32:10.292535Z"},"trusted":true},"outputs":[],"source":["!pip install torchsummary"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T16:32:28.356370Z","iopub.status.busy":"2024-04-14T16:32:28.355707Z","iopub.status.idle":"2024-04-14T16:32:29.622477Z","shell.execute_reply":"2024-04-14T16:32:29.621406Z","shell.execute_reply.started":"2024-04-14T16:32:28.356336Z"},"trusted":true},"outputs":[],"source":["from torchsummary import summary\n","model = eca_resnet18()\n","summary(model.cuda(), (1, 128, 128))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T08:42:46.977140Z","iopub.status.busy":"2024-04-24T08:42:46.976058Z","iopub.status.idle":"2024-04-24T08:42:47.172716Z","shell.execute_reply":"2024-04-24T08:42:47.171700Z","shell.execute_reply.started":"2024-04-24T08:42:46.977106Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(0)\n","def checksum(model):\n","    s = torch.sum(torch.stack([p.double().abs().sum() for p in model.parameters()]))\n","    return s\n","\n","for _ in range(10):\n","    torch.manual_seed(0)\n","    model = eca_resnet18()\n","#     dd=domain_disc()\n","    s = checksum(model)\n","#     s1=checksum(dd)\n","    print(s)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T15:00:19.010001Z","iopub.status.busy":"2024-04-21T15:00:19.009316Z","iopub.status.idle":"2024-04-21T15:00:19.064389Z","shell.execute_reply":"2024-04-21T15:00:19.063305Z","shell.execute_reply.started":"2024-04-21T15:00:19.009973Z"},"trusted":true},"outputs":[],"source":["if torch.cuda.is_available():\n","        device = \"cuda\"\n","else:\n","        device = \"cpu\"\n","print(f\"Using device {device}\")"]},{"cell_type":"markdown","metadata":{},"source":["Metric Functions are described below:\n","    -> t-SNE diagram for feature visualization\n","    -> DET curve \n","    -> Equal Error Rate Computation\n","    -> Normalized Mutual Information Calculation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot t-SNE embeddings\n","import numpy as np\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","import sklearn.metrics\n","from sklearn.metrics import det_curve, DetCurveDisplay\n","from sklearn.cluster import KMeans\n","from sklearn.metrics.cluster import normalized_mutual_info_score\n","\n","#t-SNE diagram for visualizing impact on feature space\n","def tsned(fet_a,label_a, domain_a):\n","    features_np = np.concatenate([tensor.detach().cpu().numpy() for tensor in fet_a], axis=0)\n","    domains_np = np.concatenate([tensor.detach().cpu().numpy() for tensor in domain_a], axis=0)\n","    targets_np = np.concatenate([tensor.detach().cpu().numpy() for tensor in label_a], axis=0)\n","    data = np.column_stack((features_np, domains_np, targets_np))\n","    features = data[:, :-2]\n","    domains = data[:, -2].astype(int)\n","    spoof_or_genuine = data[:, -1].astype(int)\n","    \n","#     pca_50 = PCA(n_components=50)\n","#     pca_result_50 = pca_50.fit_transform(features)\n","\n","    # Compute t-SNE embeddings\n","    tsne = TSNE(n_components=2, random_state=42)\n","    embeddings = tsne.fit_transform(features)\n","    plt.figure(figsize=(8, 6))\n","    domain_colors = {domain: color for domain, color in zip(np.unique(domains), plt.cm.tab10.colors)}\n","    \n","    m=(spoof_or_genuine == 0)\n","    genuine_embed1=embeddings[m,0]\n","    genuine_embed2=embeddings[m,1]\n","\n","    markers = {0: 'o', 1: '^'}\n","\n","    for domain in np.unique(domains):\n","        domain_mask = (domains == domain)\n","        color = domain_colors[domain]\n","        for spoof in np.unique(spoof_or_genuine):\n","            spoof_mask = (spoof_or_genuine == spoof)\n","            mask = domain_mask & spoof_mask\n","            marker = markers[spoof]\n","            edgecolor = 'k' if marker == '^' else None  # Set edge color only for '^' marker\n","            plt.scatter(embeddings[mask, 0], embeddings[mask, 1],\n","                        label=f\"Domain {domain}, {'Spoof' if spoof == 1 else 'Genuine'}\",\n","                        color=color, marker=marker, edgecolors=edgecolor)\n","\n","    plt.title('t-SNE plot with domains and spoof/genuine')\n","    plt.xlabel('t-SNE Component 1')\n","    plt.ylabel('t-SNE Component 2')\n","    plt.legend()\n","    plt.show()\n","\n","#Equal Error Rate computation \n","def compute_eer(label, pred, positive_label=1):\n","    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred)\n","    fnr = 1 - tpr\n","\n","    # the threshold of fnr == fpr\n","    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n","\n","    # theoretically eer from fpr and eer from fnr should be identical but they can be slightly differ in reality\n","    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n","    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n","\n","    # return the mean of eer from fpr and from fnr\n","    eer = (eer_1 + eer_2) / 2\n","    return eer\n","\n","#DET Curve \n","def plot_det(y_true,y_pred):\n","    fpr, fnr, _ = det_curve(y_true, y_pred)\n","    display = DetCurveDisplay(fpr=fpr, fnr=fnr)\n","    display.plot()\n","    plt.title('DET Curve')\n","    plt.legend()\n","    plt.show()\n","\n","# Calculation of normalised mutual information score sue to domain generalization methods\n","# domain_a here refers to triplet labels\n","def cluster_triplet(fet_a,label_a, domain_a):\n","    features_np = np.concatenate([tensor.detach().cpu().numpy() for tensor in fet_a], axis=0)\n","    domains_np = np.concatenate([tensor.detach().cpu().numpy() for tensor in domain_a], axis=0)\n","    targets_np = np.concatenate([tensor.detach().cpu().numpy() for tensor in label_a], axis=0)\n","    data = np.column_stack((features_np, domains_np, targets_np))\n","    # Separate features, domains, and spoof_or_genuine\n","    features = data[:, :-2]\n","    domains = data[:, -2].astype(int)\n","    spoof_or_genuine = data[:, -1].astype(int)\n","    tsne = TSNE(n_components=2, random_state=42)\n","    embeddings = tsne.fit_transform(features)\n","    pred= KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit_predict(embeddings)\n","    score=normalized_mutual_info_score(domains, pred)\n","    return score\n","    \n","    \n","    \n","    "]},{"cell_type":"markdown","metadata":{},"source":["TRAIN DATASET CLASS \n","gets the melspectrogram feature, corresponding fake speech label, domain label and triplet label (used for triplet mining)\n","annot : npy file containing labels\n","audio_dir : npy files containing features\n","domain_path : npy files containing domain labels\n","(npy file containing triplet labels is imported in the class itself)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T21:35:26.550409Z","iopub.status.busy":"2024-04-25T21:35:26.549701Z","iopub.status.idle":"2024-04-25T21:35:26.564679Z","shell.execute_reply":"2024-04-25T21:35:26.563799Z","shell.execute_reply.started":"2024-04-25T21:35:26.550379Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset\n","import pandas as pd\n","import torchaudio\n","\n","\n","class UrbanSoundDataset(Dataset):\n","\n","    def __init__(self,annot,domain_path,\n","                 audio_dir,\n","                 device):\n","        self.annotations = np.load(annot)\n","        self.domain_path=np.load(domain_path)\n","        self.triplet=np.load(\"/kaggle/input/melspec/MEL_T.npy\")\n","        self.x=np.load(audio_dir)\n","        self.files=np.expand_dims(self.x,axis=1)\n","        self.audio_dir = torch.tensor(self.files,dtype=torch.float32)\n","        self.device = device\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, index):\n","        label = torch.tensor(self._get_audio_sample_label(index),dtype=torch.int64)\n","        # label = self.label_to_numeric(label)\n","        feat=self._get_audio_sample_feature(index)\n","        domain=torch.tensor(self._get_audio_sample_domain(index))\n","        triplet_label=torch.tensor(self._get_audio_sample_triplet(index))\n","        return feat, label,domain,triplet_label\n","\n","    def label_to_numeric(self,label):\n","        if label == 'genuine':\n","            return 0\n","        elif label == 'spoof':\n","            return 1\n","        else:\n","            raise ValueError(\"Invalid label\")\n","\n","    def _get_audio_sample_label(self, index):\n","        return self.annotations[index]\n","    def _get_audio_sample_domain(self, index):\n","        return self.domain_path[index]\n","    def _get_audio_sample_triplet(self, index):\n","        return self.triplet[index]\n","\n","    def _get_audio_sample_feature(self, index):\n","        return self.audio_dir[index]\n","\n","if __name__ == \"__main__\":\n","    ANNOTATIONS_FILE = \"/kaggle/input/melspec/MEL_y.npy\"\n","    AUDIO_DIR = \"/kaggle/input/melspec/MEL_X.npy\"\n","    DOMAIN_PATH=\"/kaggle/input/melspec/MEL_D.npy\"\n","\n","\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    else:\n","        device = \"cpu\"\n","    print(f\"Using device {device}\")"]},{"cell_type":"markdown","metadata":{},"source":["TEST DATASET CLASS \n","gets the mespectrogram features ad corresponding fake speech labels\n","annot : npy file with fake speech labels\n","x_path : npy file with features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T18:06:37.684940Z","iopub.status.busy":"2024-04-21T18:06:37.684567Z","iopub.status.idle":"2024-04-21T18:06:37.699159Z","shell.execute_reply":"2024-04-21T18:06:37.698093Z","shell.execute_reply.started":"2024-04-21T18:06:37.684908Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset\n","import pandas as pd\n","import torchaudio\n","\n","class Test(Dataset):\n","\n","    def __init__(self,annot,\n","                 audio_dir,x_path,\n","                 device):\n","\n","        self.annotations = np.load(annot)\n","        self.x=np.load(x_path)\n","        self.files=np.expand_dims(self.x,axis=1)\n","        self.audio_dir = torch.tensor(self.files,dtype=torch.float32)\n","        self.device = device\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, index):\n","        # audio_sample_path = self._get_audio_sample_path(index)\n","        label = torch.tensor(self._get_audio_sample_label(index))\n","        # label = self.label_to_numeric(label)\n","        feat=self._get_audio_sample_feature(index)\n","        return feat, label\n","\n","    def label_to_numeric(self,label):\n","        if label == 'genuine':\n","            return 0\n","        elif label == 'spoof':\n","            return 1\n","        else:\n","            raise ValueError(\"Invalid label\")\n","\n","    def _get_audio_sample_label(self, index):\n","        return self.annotations[index]\n","\n","    def _get_audio_sample_feature(self, index):\n","        return self.audio_dir[index]\n","\n","if __name__ == \"__main__\":\n","\n","    audio_dir = \"/kaggle/input/melspec/MEL_X.npy\"\n","\n","\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    else:\n","        device = \"cpu\"\n","    print(f\"Using device {device}\")"]},{"cell_type":"markdown","metadata":{},"source":["Intializing test datasets"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T18:08:49.725046Z","iopub.status.busy":"2024-04-21T18:08:49.724274Z","iopub.status.idle":"2024-04-21T18:09:03.395293Z","shell.execute_reply":"2024-04-21T18:09:03.394219Z","shell.execute_reply.started":"2024-04-21T18:08:49.725008Z"},"trusted":true},"outputs":[],"source":["test_pa=Test(\"/kaggle/input/melspec/pa_y.npy\",audio_dir,\"/kaggle/input/melspec/PA_EVAL_MEL_STACK_X.npy\",device)\n","test_la=Test(\"/kaggle/input/melspec/la_y.npy\",audio_dir,\"/kaggle/input/melspec/LA_EVAL_MEL_STACK_X.npy\",device)\n","test_itw=Test(\"/kaggle/input/melspec/itw_y.npy\",audio_dir,\"/kaggle/input/melspec/ITWW_MEL_STACK_X.npy\",device)"]},{"cell_type":"markdown","metadata":{},"source":["TESTING\n","ouputs accuracy, equal error rate, confusion matrix and det curve"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T20:17:22.436997Z","iopub.status.busy":"2024-04-21T20:17:22.436496Z","iopub.status.idle":"2024-04-21T20:17:23.896565Z","shell.execute_reply":"2024-04-21T20:17:23.895122Z","shell.execute_reply.started":"2024-04-21T20:17:22.436962Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","def create_test_data_loader(test_data, batch_size):\n","    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n","    return test_dataloader\n","softmax = nn.Softmax(dim=1)\n","BATCH_SIZE = 128\n","if __name__ == \"__main__\":\n","    test_dataloader = create_test_data_loader(test_la, BATCH_SIZE)\n","    model_ev.eval()  # Set the model to evaluation mode\n","    val_loss = 0.0\n","    val_steps = 0\n","    total_val = 0\n","    correct_val = 0\n","    y_pred_v=[]\n","    y_true_v=[]\n","    y_one_v=[]\n","    val_ax=[]\n","    with torch.no_grad():  \n","        for inp, target in test_dataloader:\n","            inp, target = inp.to(device), target.to(device)\n","            middle_output1,middle_output2,middle_output3,prediction,features = model_ev(inp)\n","            probabilities = softmax(prediction)\n","            label_1_probabilities = probabilities[:, 1]\n","            predicted_classes = torch.argmax(probabilities, dim=1)\n","            correct_val += (predicted_classes == target).sum().item()\n","            total_val += target.size(0)\n","            y_pred_v.extend(predicted_classes.data.cpu().numpy())\n","            y_true_v.extend(target.data.cpu().numpy())\n","            y_one_v.extend(label_1_probabilities.data.cpu().numpy())           \n","            val_steps += 1\n","    test_accuracy = correct_val / total_val\n","    print(\"test  Accuracy = {}\".format(test_accuracy))\n","    val_ax.append(test_accuracy)\n","    eer=compute_eer(y_true_v,y_one_v)\n","    print(\"test eer = {}\".format(eer))\n","    classes = ('genuine','spoof')\n","    cf_matrix2 = confusion_matrix(y_true_v, y_pred_v)\n","    df_cm2 = pd.DataFrame(cf_matrix2 / np.sum(cf_matrix2, axis=1)[:, None], index = [i for i in classes],\n","                     columns = [i for i in classes])\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm2, annot=True)\n","    plot_det(y_true_v,y_one_v)\n","    la2=y_pred_v\n","    \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["COHEN's KAPPA SCORE (between normal base model and proposed model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-15T20:57:57.783369Z","iopub.status.busy":"2024-04-15T20:57:57.782980Z","iopub.status.idle":"2024-04-15T20:57:57.813168Z","shell.execute_reply":"2024-04-15T20:57:57.812306Z","shell.execute_reply.started":"2024-04-15T20:57:57.783337Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import cohen_kappa_score\n","la_cohe=cohen_kappa_score(sd_la, la)\n","pa_cohe=cohen_kappa_score(sd_pa, pa)\n","itw_cohe=cohen_kappa_score(sd_itw, itw)\n","print(\"la \",la_cohe)\n","print(\"pa \",pa_cohe)\n","print(\"itwa \",itw_cohe)"]},{"cell_type":"markdown","metadata":{},"source":["HYPERPARAMETER TUNING WITH OPTUNA\n","training,validation curve, confusion matrix : training,validation and t-SNE diagram on validation data visualized"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torchaudio\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import optuna\n","from optuna.trial import TrialState\n","from torch.utils.data import random_split\n","from matplotlib.pylab import plt\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","BATCH_SIZE = 128\n","EPOCHS = 20\n","# LEARNING_RATE = 0.001\n","softmax = nn.Softmax(dim=1)\n","\n","def create_data_loader(train_data, batch_size):\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size,shuffle=True,generator=g)\n","    return train_dataloader\n","\n","def kd_loss_function(output, target_output,temperature):\n","#     temperature=3\n","    output = output / temperature\n","    output_log_softmax = torch.log_softmax(output, dim=1)\n","    loss_kd = -torch.mean(torch.sum(output_log_softmax * target_output, dim=1))\n","    return loss_kd\n","\n","\n","def train_single_epoch(trial,model, data_loader,val_loader, loss_fn,optimiser, device,i,alpha,train_losses,val_losses,temperature):\n","#     temperature=3\n","    correct=0\n","    fet_a=[]\n","    label_a=[]\n","    domain_a=[]\n","    total=0\n","    y_pred_tr=[]\n","    y_true_tr=[]\n","    total_train_loss=0\n","    total_steps=0\n","    model.train()\n","    for inp, target,domain,tr in data_loader:\n","        inp = inp.to(device)\n","        target=target.to(device)\n","        domain=domain.to(device)\n","        tr=tr.to(device)\n","        middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","        probabilities = softmax(prediction.detach())\n","        predicted_classes = torch.argmax(probabilities, dim=1)\n","        correct += (predicted_classes == target).sum().item()\n","        total += target.size(0)\n","        y_pred_tr.extend(predicted_classes.data.cpu().numpy())\n","        y_true_tr.extend(target.data.cpu().numpy())\n","        total_loss = loss_fn(prediction, target)\n","        total_train_loss+=total_loss.detach().cpu().numpy()\n","        total_steps+=1\n","        optimiser.zero_grad()\n","        total_loss.backward()\n","        optimiser.step()\n","    train_losses.append(total_train_loss/total_steps)\n","    print(f\"train loss: {total_loss.item()}\")\n","    accuracy = correct / total\n","    print(\"train   Accuracy = {}\".format(accuracy))\n","\n","    val_loss = 0.0\n","    val_steps = 0\n","    total_val = 0\n","    correct_val = 0\n","    y_pred_v=[]\n","    y_true_v=[]\n","    fet_a=[]\n","    label_a=[]\n","    domain_a=[]\n","    model.eval()\n","    for inp, target,domain,tr in val_loader:\n","            with torch.no_grad():\n","                inp=inp.to(device)\n","                target=target.to(device)\n","                doamin=domain.to(device)\n","                tr=tr.to(device)\n","                middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","                fet_a.append(features)\n","                label_a.append(target)\n","                domain_a.append(domain)\n","                probabilities = softmax(prediction)\n","                predicted_classes = torch.argmax(probabilities, dim=1)\n","                correct_val += (predicted_classes == target).sum().item()\n","                total_val += target.size(0)\n","                y_pred_v.extend(predicted_classes.data.cpu().numpy())\n","                y_true_v.extend(target.data.cpu().numpy())\n","                total_loss_val = loss_fn(prediction, target)          \n","                val_loss += total_loss_val.detach().cpu().numpy()\n","                val_steps += 1\n","    val_accuracy = correct_val / total_val\n","    print(f\"val loss: {total_loss_val.item()}\")\n","    val_losses.append(val_loss/val_steps)\n","    print(\"val  Accuracy = {}\".format(val_accuracy))\n","    trial.report(val_accuracy, i)\n","    if trial.should_prune():\n","        raise optuna.exceptions.TrialPruned()\n","    return val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a\n","\n","def train(trial):\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    else:\n","        device = \"cpu\"\n","    print(f\"Using {device}\")\n","    epochs=20\n","    torch.manual_seed(0)\n","    usd = UrbanSoundDataset(ANNOTATIONS_FILE,DOMAIN_PATH,AUDIO_DIR,\n","                            device)\n","    test_abs = int(len(usd) * 0.8)\n","    train_subset, val_subset = random_split(usd, [test_abs, len(usd) - test_abs],generator=g)\n","    train_dataloader = create_data_loader(train_subset, BATCH_SIZE)\n","    val_loader=create_data_loader(val_subset,BATCH_SIZE)\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    model = eca_resnet18().to(device)\n","    lr = trial.suggest_float(\"lr\", 0.01, 0.03, log=True)\n","    # alpha=0.86\n","    # temperature=3\n","    optimiser = torch.optim.SGD(model.parameters(),\n","                                 lr=lr)\n","    train_losses=[]\n","    val_losses=[]\n","    for i in range(epochs):\n","        print(f\"Epoch {i+1}\")\n","        val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a=train_single_epoch(trial,model, train_dataloader,val_loader,loss_fn, optimiser, device,i,alpha,train_losses,val_losses,temperature)\n","        print(\"---------------------------\")\n","    print(\"Finished training\")\n","    epochs_plot = range(1, 21)\n","    plt.plot(epochs_plot, train_losses, label='Training Loss')\n","    plt.plot(epochs_plot, val_losses, label='Validation Loss')\n","    classes = ('genuine','spoof')\n","    cf_matrix1 = confusion_matrix(y_true_tr, y_pred_tr)\n","    cf_matrix2 = confusion_matrix(y_true_v, y_pred_v)\n","    df_cm1 = pd.DataFrame(cf_matrix1 / np.sum(cf_matrix1, axis=1)[:, None], index = [i for i in classes],\n","                     columns = [i for i in classes])\n","    df_cm2 = pd.DataFrame(cf_matrix2 / np.sum(cf_matrix2, axis=1)[:, None], index = [i for i in classes],\n","                     columns = [i for i in classes])\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm1, annot=True)\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm2, annot=True)\n","    tsned(fet_a,label_a,domain_a)\n","    return val_accuracy\n","\n","\n","if __name__ == \"__main__\":\n","\n","    study = optuna.create_study(direction=\"maximize\")\n","    study.optimize(train, n_trials=10)\n","    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n","    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n","\n","    print(\"Study statistics: \")\n","    print(\"  Number of finished trials: \", len(study.trials))\n","    print(\"  Number of pruned trials: \", len(pruned_trials))\n","    print(\"  Number of complete trials: \", len(complete_trials))\n","\n","    print(\"Best trial:\")\n","    trial = study.best_trial\n","\n","    print(\"  Value: \", trial.value)\n","\n","    print(\"  Params: \")\n","    # for key, value in trial.params.items():\n","    #     print(\"    {}: {}\".format(key, value))\n","    # torch.save(model.state_dict(), \"state_sd.pth\")\n","    # print(\"Trained feed forward net saved at feedforwardnet.pth\")"]},{"cell_type":"markdown","metadata":{},"source":["Base Model ( without any of the proposed frameowrk)\n","learning rate (variable lr ): replace with best learning rate obtained through hyperparameter tuning\n","Outputs : displayed accuracy, loss and eer over the epochs, training and validation loss, accuracy,det curves and confusion matrices,\n","t-SNE diagram on validation set\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T18:26:20.413049Z","iopub.status.busy":"2024-04-21T18:26:20.412706Z","iopub.status.idle":"2024-04-21T18:29:17.734804Z","shell.execute_reply":"2024-04-21T18:29:17.733926Z","shell.execute_reply.started":"2024-04-21T18:26:20.413023Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchaudio\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import optuna\n","from optuna.trial import TrialState\n","from torch.utils.data import random_split\n","from matplotlib.pylab import plt\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","BATCH_SIZE = 128\n","EPOCHS = 20\n","# LEARNING_RATE = 0.001\n","softmax = nn.Softmax(dim=1)\n","\n","def create_data_loader(train_data, batch_size):\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size,shuffle=True)\n","    return train_dataloader\n","\n","\n","def train_single_epoch(model, data_loader,val_loader, loss_fn,optimiser, device,i,alpha,train_losses,val_losses,temperature,train_ax,val_ax):\n","# model training \n","    correct=0\n","    fet_a=[]\n","    label_a=[]\n","    domain_a=[]\n","    total=0\n","    y_pred_tr=[]\n","    y_true_tr=[]\n","    y_one_tr=[]\n","    total_steps=0\n","    total_train_loss=0\n","    model.train()\n","    for inp, target,domain,tr in data_loader:\n","        inp = inp.to(device)\n","        target=target.to(device)\n","        domain=domain.to(device)\n","        tr=tr.to(device)\n","        middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","        probabilities = softmax(prediction.detach())\n","        label_1_probabilities = probabilities[:, 1]\n","        predicted_classes = torch.argmax(probabilities, dim=1)\n","        correct += (predicted_classes == target).sum().item()\n","        total += target.size(0)\n","        y_pred_tr.extend(predicted_classes.data.cpu().numpy())\n","        y_one_tr.extend(label_1_probabilities.data.cpu().numpy())\n","        y_true_tr.extend(target.data.cpu().numpy())\n","        # losses3_kd.update(loss3by4, input.size(0))\n","        loss = loss_fn(prediction, target)\n","        optimiser.zero_grad()\n","        loss.backward()\n","        optimiser.step()\n","        total_train_loss+=loss.detach().cpu().numpy()\n","        total_steps+=1\n","\n","    train_losses.append(total_train_loss/total_steps)\n","    # print(f\"train loss: {total_train_loss/total_steps.item()}\")\n","    accuracy = correct / total\n","    train_ax.append(accuracy)\n","    eer=compute_eer(y_true_tr,y_one_tr)\n","    print(\"train eer = {}\".format(eer))\n","\n","\n","    # validation set\n","    val_loss = 0.0\n","    val_steps = 0\n","    total_val = 0\n","    correct_val = 0\n","    y_pred_v=[]\n","    y_true_v=[]\n","    fet_a=[]\n","    label_a=[]\n","    domain_a=[]\n","    y_one_v=[]\n","    model.eval()\n","    for inp, target,domain,tr in val_loader:\n","            with torch.no_grad():\n","                inp=inp.to(device)\n","                target=target.to(device)\n","                doamin=domain.to(device)\n","                tr=tr.to(device)\n","                middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","                fet_a.append(features)\n","                label_a.append(target)\n","                domain_a.append(domain)\n","                probabilities = softmax(prediction)\n","                label_1_probabilities = probabilities[:, 1]\n","                predicted_classes = torch.argmax(probabilities, dim=1)\n","                correct_val += (predicted_classes == target).sum().item()\n","                total_val += target.size(0)\n","                y_pred_v.extend(predicted_classes.data.cpu().numpy())\n","                y_true_v.extend(target.data.cpu().numpy())\n","                y_one_v.extend(label_1_probabilities.data.cpu().numpy())\n","                loss = loss_fn(prediction, target)            \n","                val_loss += loss.detach().cpu().numpy()\n","                val_steps += 1\n","    val_accuracy = correct_val / total_val\n","    # print(f\"val loss: {val_loss/val_steps.item()}\")\n","    val_losses.append(val_loss/val_steps)\n","    print(\"val  Accuracy = {}\".format(val_accuracy))\n","    val_ax.append(val_accuracy)\n","    eer=compute_eer(y_true_v,y_one_v)\n","    print(\"train eer = {}\".format(eer))\n","    return val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a,y_one_tr,y_one_v\n","\n","def train():\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    else:\n","        device = \"cpu\"\n","    print(f\"Using {device}\")\n","    epochs=20\n","    usd = UrbanSoundDataset(ANNOTATIONS_FILE,DOMAIN_PATH,AUDIO_DIR,\n","                            device)\n","    test_abs = int(len(usd) * 0.8)\n","    train_subset, val_subset = random_split(usd, [test_abs, len(usd) - test_abs],generator=torch.Generator().manual_seed(500))\n","    train_dataloader = create_data_loader(train_subset, BATCH_SIZE)\n","    val_loader=create_data_loader(val_subset,BATCH_SIZE)\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    torch.manual_seed(0)\n","    model = eca_resnet18().to(device)\n","    optimiser = torch.optim.SGD(model.parameters(),\n","                                 lr=lr)\n","    temperature=3\n","    train_losses=[]\n","    val_losses=[]\n","    train_ax=[]\n","    val_ax=[]\n","    for i in range(epochs):\n","        print(f\"Epoch {i+1}\")\n","        val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a,y_one_tr,y_one_v=train_single_epoch(model, train_dataloader,val_loader,loss_fn, optimiser, device,i,alpha,train_losses,val_losses,temperature,train_ax,val_ax)\n","        print(\"---------------------------\")\n","    print(\"Finished training\")\n","    epochs_plot = range(1, 21)\n","    plt.plot(epochs_plot, train_losses, label='Training Loss')\n","    plt.plot(epochs_plot, val_losses, label='Validation Loss')\n","    classes = ('genuine','spoof')\n","    cf_matrix1 = confusion_matrix(y_true_tr, y_pred_tr)\n","    cf_matrix2 = confusion_matrix(y_true_v, y_pred_v)\n","    df_cm1 = pd.DataFrame(cf_matrix1 / np.sum(cf_matrix1, axis=1)[:, None], index = [i for i in classes],\n","                     columns = [i for i in classes])\n","    df_cm2 = pd.DataFrame(cf_matrix2 / np.sum(cf_matrix2, axis=1)[:, None], index = [i for i in classes],\n","                     columns = [i for i in classes])\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm1, annot=True)\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm2, annot=True)\n","    tsned(fet_a,label_a,domain_a)\n","    plot_det(y_true_tr,y_one_tr)\n","    plot_det(y_true_v,y_one_v)\n","    plt.plot(epochs_plot, train_ax, label='Training Accuracy')\n","    plt.plot(epochs_plot, val_ax, label='Validation Accuracy')\n","    return y_pred_v,model\n","\n","\n","if __name__ == \"__main__\":\n","\n","    y_pred,model2=train()\n","    torch.save(model2.state_dict(), \"base_model.pth\")"]},{"cell_type":"markdown","metadata":{},"source":["**with self distillation**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-21T18:22:53.586862Z","iopub.status.busy":"2024-04-21T18:22:53.586464Z","iopub.status.idle":"2024-04-21T18:25:59.478031Z","shell.execute_reply":"2024-04-21T18:25:59.477096Z","shell.execute_reply.started":"2024-04-21T18:22:53.586832Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchaudio\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import optuna\n","from optuna.trial import TrialState\n","from torch.utils.data import random_split\n","from matplotlib.pylab import plt\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","BATCH_SIZE = 128\n","EPOCHS = 20\n","# LEARNING_RATE = 0.001\n","softmax = nn.Softmax(dim=1)\n","\n","def create_data_loader(train_data, batch_size):\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size,shuffle=True)\n","    return train_dataloader\n","\n","def kd_loss_function(output, target_output,temperature):\n","#     temperature=3\n","    output = output / temperature\n","    output_log_softmax = torch.log_softmax(output, dim=1)\n","    loss_kd = -torch.mean(torch.sum(output_log_softmax * target_output, dim=1))\n","    return loss_kd\n","\n","\n","def train_single_epoch(model, data_loader,val_loader, loss_fn,optimiser, device,i,alpha,train_losses,val_losses,temperature,train_ax,val_ax):\n","#     temperature=3\n","    correct=0\n","    fet_a=[]\n","    label_a=[]\n","    domain_a=[]\n","    total=0\n","    y_pred_tr=[]\n","    y_true_tr=[]\n","    y_one_tr=[]\n","    total_steps=0\n","    total_train_loss=0\n","    model.train()\n","    for inp, target,domain,tr in data_loader:\n","        inp = inp.to(device)\n","        target=target.to(device)\n","        domain=domain.to(device)\n","        tr=tr.to(device)\n","        middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","        temp4 = prediction / temperature\n","        temp4 = torch.softmax(temp4, dim=1)\n","        probabilities = softmax(prediction.detach())\n","        label_1_probabilities = probabilities[:, 1]\n","        predicted_classes = torch.argmax(probabilities, dim=1)\n","        correct += (predicted_classes == target).sum().item()\n","        total += target.size(0)\n","        y_pred_tr.extend(predicted_classes.data.cpu().numpy())\n","        y_one_tr.extend(label_1_probabilities.data.cpu().numpy())\n","        y_true_tr.extend(target.data.cpu().numpy())\n","        loss1by4 = kd_loss_function(middle_output1, temp4.detach(),temperature)* (temperature**2)\n","        loss2by4 = kd_loss_function(middle_output2, temp4.detach(),temperature) * (temperature**2)\n","        loss3by4 = kd_loss_function(middle_output3, temp4.detach(),temperature) * (temperature**2)\n","        loss = loss_fn(prediction, target)\n","        total_loss = (1 - alpha) * (loss1by4 + loss2by4 + loss3by4) +  alpha *loss\n","        optimiser.zero_grad()\n","        total_loss.backward()\n","        optimiser.step()\n","        total_train_loss+=total_loss.detach().cpu().numpy()\n","        total_steps+=1\n","        # correct += (thresholded_pred_acc == target).float().sum()\n","    train_losses.append(total_train_loss/total_steps)\n","    tll=total_train_loss/total_steps\n","    print(f\"train loss: {tll}\")\n","    accuracyt = correct / total\n","    eer=compute_eer(y_true_tr,y_one_tr)\n","    print(\"train eer = {}\".format(eer))\n","    print(\"train  Accuracy = {}\".format(accuracyt))\n","    train_ax.append(accuracyt)\n","\n","    val_loss = 0.0\n","    val_steps = 0\n","    total_val = 0\n","    correct_val = 0\n","    y_pred_v=[]\n","    y_true_v=[]\n","    fet_a=[]\n","    label_a=[]\n","    domain_a=[]\n","    y_one_v=[]\n","    model.eval()\n","    for inp, target,domain,tr in val_loader:\n","            with torch.no_grad():\n","                inp=inp.to(device)\n","                target=target.to(device)\n","                doamin=domain.to(device)\n","                tr=tr.to(device)\n","                middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","                fet_a.append(features)\n","                label_a.append(target)\n","                domain_a.append(domain)\n","                temp4 = prediction / temperature\n","                temp4 = torch.softmax(temp4, dim=1)\n","                probabilities = softmax(prediction)\n","                label_1_probabilities = probabilities[:, 1]\n","                predicted_classes = torch.argmax(probabilities, dim=1)\n","                correct_val += (predicted_classes == target).sum().item()\n","                total_val += target.size(0)\n","                y_pred_v.extend(predicted_classes.data.cpu().numpy())\n","                y_true_v.extend(target.data.cpu().numpy())\n","                y_one_v.extend(label_1_probabilities.data.cpu().numpy())\n","                loss1by4 = kd_loss_function(middle_output1, temp4.detach(),temperature)* (temperature**2)\n","                loss2by4 = kd_loss_function(middle_output2, temp4.detach(),temperature) * (temperature**2)\n","                loss3by4 = kd_loss_function(middle_output3, temp4.detach(),temperature) * (temperature**2)\n","                loss = loss_fn(prediction, target)\n","                total_loss_val = (1 - alpha) * (loss1by4 + loss2by4 + loss3by4) + alpha *loss            \n","                val_loss += total_loss_val.detach().cpu().numpy()\n","                val_steps += 1\n","    val_accuracy = correct_val / total_val\n","    vll=val_loss/val_steps\n","    print(\"val loss: {}\".format(val_loss/val_steps))\n","    val_losses.append(val_loss/val_steps)\n","    print(\"val  Accuracy = {}\".format(val_accuracy))\n","    val_ax.append(val_accuracy)\n","    eer=compute_eer(y_true_v,y_one_v)\n","    print(\"train eer = {}\".format(eer))\n","    return val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a,y_one_tr,y_one_v\n","\n","def train():\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    else:\n","        device = \"cpu\"\n","    print(f\"Using {device}\")\n","    epochs=20\n","    usd = UrbanSoundDataset(ANNOTATIONS_FILE,DOMAIN_PATH,AUDIO_DIR,\n","                            device)\n","    test_abs = int(len(usd) * 0.8)\n","    train_subset, val_subset = random_split(usd, [test_abs, len(usd) - test_abs],generator=torch.Generator().manual_seed(500))\n","    train_dataloader = create_data_loader(train_subset, BATCH_SIZE)\n","    val_loader=create_data_loader(val_subset,BATCH_SIZE)\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    torch.manual_seed(0)\n","    model = eca_resnet18().to(device)\n","    optimiser = torch.optim.SGD(model.parameters(),\n","                                 lr=lr)\n","    alpha=0.86\n","    temperature=3\n","    train_losses=[]\n","    val_losses=[]\n","    train_ax=[]\n","    val_ax=[]\n","    for i in range(epochs):\n","        print(f\"Epoch {i+1}\")\n","        val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a,y_one_tr,y_one_v=train_single_epoch(model, train_dataloader,val_loader,loss_fn, optimiser, device,i,alpha,train_losses,val_losses,temperature,train_ax,val_ax)\n","        print(\"---------------------------\")\n","    print(\"Finished training\")\n","    epochs_plot = range(1, 21)\n","    plt.plot(epochs_plot, train_losses, label='Training Loss')\n","    plt.plot(epochs_plot, val_losses, label='Validation Loss')\n","    classes = ('genuine','spoof')\n","    cf_matrix1 = confusion_matrix(y_true_tr, y_pred_tr)\n","    cf_matrix2 = confusion_matrix(y_true_v, y_pred_v)\n","    df_cm1 = pd.DataFrame(cf_matrix1 / np.sum(cf_matrix1, axis=1)[:, None], index = [i for i in classes],\n","                     columns = [i for i in classes])\n","    df_cm2 = pd.DataFrame(cf_matrix2 / np.sum(cf_matrix2, axis=1)[:, None], index = [i for i in classes],\n","                     columns = [i for i in classes])\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm1, annot=True)\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm2, annot=True)\n","    tsned(fet_a,label_a,domain_a)\n","    plot_det(y_true_tr,y_one_tr)\n","    plot_det(y_true_v,y_one_v)\n","    plt.plot(epochs_plot, train_ax, label='Training Accuracy')\n","    plt.plot(epochs_plot, val_ax, label='Validation Accuracy')\n","    return y_pred_v,model\n","\n","\n","if __name__ == \"__main__\":\n","\n","    y_pred_sd,model=train()\n","    torch.save(model.state_dict(), \"self_distil_model.pth\")"]},{"cell_type":"markdown","metadata":{},"source":["Effect of noise on models "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T20:23:04.399738Z","iopub.status.busy":"2024-04-25T20:23:04.399254Z","iopub.status.idle":"2024-04-25T20:38:26.765569Z","shell.execute_reply":"2024-04-25T20:38:26.764782Z","shell.execute_reply.started":"2024-04-25T20:23:04.399711Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchaudio\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import optuna\n","from optuna.trial import TrialState\n","from torch.utils.data import random_split\n","from matplotlib.pylab import plt\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","BATCH_SIZE = 128\n","EPOCHS = 20\n","# LEARNING_RATE = 0.001\n","softmax = nn.Softmax(dim=1)\n","sigmoid=nn.Sigmoid()\n","\n","def create_data_loader(train_data, batch_size):\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size,shuffle=True)\n","    return train_dataloader\n","\n","def add_gaussian_noise(tensor, std):\n","    noise = torch.randn(tensor.size(),device=device) * std\n","    return tensor + noise\n","\n","\n","def train_single_epoch(model, data_loader,val_loader, loss_fn,optimiser, device,i,alpha,train_losses,val_losses,temperature,train_ax,val_ax,std):\n","#     temperature=3\n","    correct=0\n","#     fet_a=[]\n","#     label_a=[]\n","#     domain_a=[]\n","    total=0\n","    y_pred_tr=[]\n","    y_true_tr=[]\n","    y_one_tr=[]\n","    total_steps=0\n","    total_train_loss=0\n","    model.train()\n","    for inp, target,domain,tr in data_loader:\n","        inp = inp.to(device)\n","        target=target.to(device)\n","        domain=domain.to(device)\n","        tr=tr.to(device)\n","        for p in model.parameters():\n","             p.data = p.data.to(device)\n","             p.data = add_gaussian_noise(p.data,std)\n","        middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","\n","        probabilities = softmax(prediction.detach())\n","        label_1_probabilities = probabilities[:, 1]\n","        predicted_classes = torch.argmax(probabilities, dim=1)\n","        correct += (predicted_classes == target).sum().item()\n","        total += target.size(0)\n","        loss = loss_fn(prediction, target)\n","        optimiser.zero_grad()\n","        loss.backward()\n","        optimiser.step()\n","        total_train_loss+=loss.detach().cpu().numpy()\n","        total_steps+=1\n","    accuracy = correct / total\n","#     train_ax.append(accuracy)\n","    print(\"train   Accuracy = {}\".format(accuracy))\n","\n","    val_loss = 0.0\n","    val_steps = 0\n","    total_val = 0\n","    correct_val = 0\n","    y_pred_v=[]\n","    y_true_v=[]\n","    fet_a=[]\n","    label_a=[]\n","    domain_a=[]\n","    y_one_v=[]\n","    model.eval()\n","    for inp, target,domain,tr in val_loader:\n","            with torch.no_grad():\n","                inp=inp.to(device)\n","                target=target.to(device)\n","                domain=domain.to(device)\n","                tr=tr.to(device)\n","                middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","                probabilities = softmax(prediction.detach())\n","                label_1_probabilities = probabilities[:, 1]\n","                predicted_classes = torch.argmax(probabilities, dim=1)\n","                correct_val += (predicted_classes == target).sum().item()\n","                total_val += target.size(0)\n","                loss = loss_fn(prediction, target)            \n","                val_loss += loss.detach().cpu().numpy()\n","                val_steps += 1\n","    val_accuracy = correct_val / total_val\n","    print(\"val  Accuracy = {}\".format(val_accuracy))\n","    return val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a,y_one_tr,y_one_v,accuracy\n","\n","def train():\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    else:\n","        device = \"cpu\"\n","    print(f\"Using {device}\")\n","    torch.manual_seed(0)\n","    epochs=20\n","    usd = UrbanSoundDataset(ANNOTATIONS_FILE,DOMAIN_PATH,AUDIO_DIR,\n","                            device)\n","    test_abs = int(len(usd) * 0.8)\n","    train_subset, val_subset = random_split(usd, [test_abs, len(usd) - test_abs],generator=torch.Generator().manual_seed(500))\n","    train_dataloader = create_data_loader(train_subset, BATCH_SIZE)\n","    val_loader=create_data_loader(val_subset,BATCH_SIZE)\n","    \n","    loss_fn=torch.nn.CrossEntropyLoss()\n","    model = eca_resent18().to(device)\n","    optimiser = torch.optim.SGD(model.parameters(),lr=lr)\n","    temperature=3\n","    train_losses=[]\n","    val_losses=[]\n","    train_ax=[]\n","    val_ax=[]\n","    acc_noise_std=[]\n","    stds=[0.01,0.02,0.03,0.04,0.05]\n","    for std in stds:\n","        for i in range(epochs):\n","            print(f\"Epoch {i+1}\")\n","            val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a,y_one_tr,y_one_v,train_acc=train_single_epoch(model, train_dataloader,val_loader,loss_fn, optimiser, device,i,alpha,train_losses,val_losses,temperature,train_ax,val_ax,std)\n","            print(\"---------------------------\")\n","        acc_noise_std.append(train_acc)\n","    return y_pred_v,model,acc_noise_std\n","\n","\n","if __name__ == \"__main__\":\n","\n","    y_pred2,model2,nom_acc=train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T20:40:05.309736Z","iopub.status.busy":"2024-04-25T20:40:05.309181Z","iopub.status.idle":"2024-04-25T20:52:06.162724Z","shell.execute_reply":"2024-04-25T20:52:06.161774Z","shell.execute_reply.started":"2024-04-25T20:40:05.309708Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchaudio\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import optuna\n","from optuna.trial import TrialState\n","from torch.utils.data import random_split\n","from matplotlib.pylab import plt\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","BATCH_SIZE = 128\n","EPOCHS = 20\n","# LEARNING_RATE = 0.001\n","softmax = nn.Softmax(dim=1)\n","\n","def create_data_loader(train_data, batch_size):\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size,shuffle=True)\n","    return train_dataloader\n","\n","def kd_loss_function(output, target_output,temperature):\n","#     temperature=3\n","    output = output / temperature\n","    output_log_softmax = torch.log_softmax(output, dim=1)\n","    loss_kd = -torch.mean(torch.sum(output_log_softmax * target_output, dim=1))\n","    return loss_kd\n","def add_gaussian_noise(tensor, std):\n","    noise = torch.randn(tensor.size(),device=device) * std\n","    return tensor + noise\n","\n","def train_single_epoch(model, data_loader,val_loader, loss_fn,optimiser, device,i,alpha,train_losses,val_losses,temperature,train_ax,val_ax,std):\n","#     temperature=3\n","    correct=0\n","    fet_a=[]\n","    label_a=[]\n","    domain_a=[]\n","    total=0\n","    y_pred_tr=[]\n","    y_true_tr=[]\n","    y_one_tr=[]\n","    total_steps=0\n","    total_train_loss=0\n","    model.train()\n","    for inp, target,domain,tr in data_loader:\n","        inp = inp.to(device)\n","        target=target.to(device)\n","        domain=domain.to(device)\n","        tr=tr.to(device)\n","        for p in model.parameters():\n","             p.data = p.data.to(device)\n","#              gaussian = Normal(loc=0, scale=torch.ones_like(p))\n","             p.data = add_gaussian_noise(p.data,std)\n","        middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","        \n","        temp4 = prediction / temperature\n","        temp4 = torch.softmax(temp4, dim=1)\n","        probabilities = softmax(prediction.detach())\n","        label_1_probabilities = probabilities[:, 1]\n","        predicted_classes = torch.argmax(probabilities, dim=1)\n","        correct += (predicted_classes == target).sum().item()\n","        total += target.size(0)\n","        y_pred_tr.extend(predicted_classes.data.cpu().numpy())\n","        y_one_tr.extend(label_1_probabilities.data.cpu().numpy())\n","        y_true_tr.extend(target.data.cpu().numpy())\n","        loss1by4 = kd_loss_function(middle_output1, temp4.detach(),temperature)* (temperature**2)\n","        loss2by4 = kd_loss_function(middle_output2, temp4.detach(),temperature) * (temperature**2)\n","        loss3by4 = kd_loss_function(middle_output3, temp4.detach(),temperature) * (temperature**2)\n","        # losses3_kd.update(loss3by4, input.size(0))\n","        loss = loss_fn(prediction, target)\n","        total_loss = (1 - alpha) * (loss1by4 + loss2by4 + loss3by4) +  alpha *loss\n","        optimiser.zero_grad()\n","        total_loss.backward()\n","        optimiser.step()\n","        total_train_loss+=total_loss.detach().cpu().numpy()\n","        total_steps+=1\n","    train_losses.append(total_train_loss/total_steps)\n","    accuracy = correct / total\n","    print(\"train   Accuracy = {}\".format(accuracy))\n","\n","    val_loss = 0.0\n","    val_steps = 0\n","    total_val = 0\n","    correct_val = 0\n","    y_pred_v=[]\n","    y_true_v=[]\n","    fet_a=[]\n","    label_a=[]\n","    domain_a=[]\n","    y_one_v=[]\n","    model.eval()\n","    for inp, target,domain,tr in val_loader:\n","            with torch.no_grad():\n","                inp=inp.to(device)\n","                target=target.to(device)\n","                doamin=domain.to(device)\n","                tr=tr.to(device)\n","                middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","                fet_a.append(features)\n","                label_a.append(target)\n","                domain_a.append(domain)\n","                temp4 = prediction / temperature\n","                temp4 = torch.softmax(temp4, dim=1)\n","                probabilities = softmax(prediction)\n","                label_1_probabilities = probabilities[:, 1]\n","                predicted_classes = torch.argmax(probabilities, dim=1)\n","                correct_val += (predicted_classes == target).sum().item()\n","                total_val += target.size(0)\n","                y_pred_v.extend(predicted_classes.data.cpu().numpy())\n","                y_true_v.extend(target.data.cpu().numpy())\n","                y_one_v.extend(label_1_probabilities.data.cpu().numpy())\n","                loss1by4 = kd_loss_function(middle_output1, temp4.detach(),temperature)* (temperature**2)\n","                loss2by4 = kd_loss_function(middle_output2, temp4.detach(),temperature) * (temperature**2)\n","                loss3by4 = kd_loss_function(middle_output3, temp4.detach(),temperature) * (temperature**2)\n","                loss = loss_fn(prediction, target)\n","                total_loss_val = (1 - alpha) * (loss1by4 + loss2by4 + loss3by4) + alpha *loss            \n","                val_loss += total_loss_val.detach().cpu().numpy()\n","                val_steps += 1\n","    val_accuracy = correct_val / total_val\n","    print(\"val  Accuracy = {}\".format(val_accuracy))\n","    return val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a,y_one_tr,y_one_v,accuracy\n","\n","def train():\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    else:\n","        device = \"cpu\"\n","    print(f\"Using {device}\")\n","    epochs=20\n","    torch.manual_seed(42)\n","    usd = UrbanSoundDataset(ANNOTATIONS_FILE,DOMAIN_PATH,AUDIO_DIR,\n","                            device)\n","    test_abs = int(len(usd) * 0.8)\n","    train_subset, val_subset = random_split(usd, [test_abs, len(usd) - test_abs],generator=torch.Generator().manual_seed(500))\n","    train_dataloader = create_data_loader(train_subset, BATCH_SIZE)\n","    val_loader=create_data_loader(val_subset,BATCH_SIZE)\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    model = cnn().to(device)\n","    optimiser = torch.optim.SGD(model.parameters(),lr=lr)\n","    alpha=al\n","    temperature=3\n","    train_losses=[]\n","    val_losses=[]\n","    train_ax=[]\n","    val_ax=[]\n","    acc_noise_std=[]\n","    stds=[0.01,0.02,0.03,0.04,0.05]\n","    for std in stds:\n","        for i in range(epochs):\n","            print(f\"Epoch {i+1}\")\n","            val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a,y_one_tr,y_one_v,train_acc=train_single_epoch(model, train_dataloader,val_loader,loss_fn, optimiser, device,i,alpha,train_losses,val_losses,temperature,train_ax,val_ax,std)\n","            print(\"---------------------------\")\n","        acc_noise_std.append(train_acc)\n","    print(\"Finished training\")\n","    epochs_plot = range(1, 21)\n","    return y_pred_v,model,acc_noise_std\n","\n","\n","if __name__ == \"__main__\":\n","\n","    y_pred_sd,model,acc_noise_std=train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T21:04:23.869538Z","iopub.status.busy":"2024-04-25T21:04:23.868628Z","iopub.status.idle":"2024-04-25T21:04:24.164390Z","shell.execute_reply":"2024-04-25T21:04:24.163532Z","shell.execute_reply.started":"2024-04-25T21:04:23.869506Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","stds=[0.01,0.02,0.03,0.04,0.05]\n","plt.plot(stds, acc_noise_std, label='Self Distillation')\n","plt.plot(stds, nom_acc, label='Non Self Distillation')\n","plt.xlabel('standard deviation of noise')\n","plt.ylabel('Accuracy')\n","\n","# Add a legend\n","plt.legend()\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["Domain Adversarial Learning\n","describes the working of gradient reverse layer and the domain discriminator"]},{"cell_type":"markdown","metadata":{},"source":["Gradient Reverse Layer"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T21:36:23.328485Z","iopub.status.busy":"2024-04-25T21:36:23.327610Z","iopub.status.idle":"2024-04-25T21:36:23.337117Z","shell.execute_reply":"2024-04-25T21:36:23.336096Z","shell.execute_reply.started":"2024-04-25T21:36:23.328452Z"},"trusted":true},"outputs":[],"source":["\n","class GradientReversalF(torch.autograd.Function):\n","  @staticmethod\n","  def forward(ctx, x, alpha):\n","    #let the input unchaged\n","    ctx.save_for_backward(alpha)\n","    return x\n","\n","  @staticmethod\n","  def backward(ctx, grad_output):\n","    #reverse the gradient by multipling -alpha\n","    alpha = ctx.saved_tensors[0]\n","    if ctx.needs_input_grad[0]:\n","      grad_output = (grad_output * (-alpha))\n","    return (grad_output, None)\n","\n","\n","class GradientReverse(nn.Module):\n","  def __init__(self, alpha, *args, **kwargs):\n","    #Reverse GR layer hook\n","    super().__init__(*args, **kwargs)\n","    self.alpha = torch.tensor(alpha, requires_grad=False)\n","    assert alpha > 0, 'alpha must be > 0'\n","    print(f\"The gradient will be multiplied by: {-alpha}\")\n","\n","  def forward(self, x):\n","    return GradientReversalF.apply(x, self.alpha)"]},{"cell_type":"markdown","metadata":{},"source":["Domain Discriminator"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T21:42:14.781517Z","iopub.status.busy":"2024-04-25T21:42:14.780808Z","iopub.status.idle":"2024-04-25T21:42:14.790766Z","shell.execute_reply":"2024-04-25T21:42:14.789692Z","shell.execute_reply.started":"2024-04-25T21:42:14.781483Z"},"trusted":true},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self,block,alpha):\n","        super(Discriminator, self).__init__()\n","        self.fc1 = nn.Linear(128 * block.expansion, 128 * block.expansion)\n","        self.fc1.weight.data.normal_(0, 0.01)\n","        self.fc1.bias.data.fill_(0.0)\n","        self.fc2 = nn.Linear(128 * block.expansion, 3)\n","        self.fc2.weight.data.normal_(0, 0.3)\n","        self.fc2.bias.data.fill_(0.0)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.3)\n","        self.grl_layer = GradientReverse(alpha)\n","\n","    def forward(self, feature):\n","        feature = self.grl_layer.forward(feature)\n","        feature = self.fc1(feature)\n","        feature = self.relu(feature)\n","        feature = self.dropout(feature)\n","        feature = self.fc2(feature)\n","        return feature\n","def domain_disc():\n","    model =Discriminator(ECABasicBlock,1)\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["Checking working of gradient reversal"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T12:03:17.093212Z","iopub.status.busy":"2024-04-13T12:03:17.092482Z","iopub.status.idle":"2024-04-13T12:03:17.102894Z","shell.execute_reply":"2024-04-13T12:03:17.102023Z","shell.execute_reply.started":"2024-04-13T12:03:17.093180Z"},"trusted":true},"outputs":[],"source":["import torch\n","rev=GradientReverse(1)\n","\n","# alpha = torch.tensor([1.])\n","\n","x = torch.tensor([4.], requires_grad=True)\n","x_rev = torch.tensor([4.], requires_grad=True)\n","\n","y = x\n","y = y*5\n","\n","y_rev=x_rev\n","y_rev = rev(y_rev)\n","y_rev = y_rev*5\n","# y_rev = y_rev*6\n","\n","\n","y.backward()\n","y_rev.backward()\n","\n","print(f'x gradient: {x.grad}') # 5\n","print(f'reversed x gradient: {x_rev.grad}') # -5\n","\n","assert x.grad==-x_rev.grad"]},{"cell_type":"markdown","metadata":{},"source":["Domain adversarial learning is described below :\n","The features outputted from the generator, here the self distilled model are fed into a domain discriminator.\n","Domain discriminator is used to predict the domains of the input data ( gradient reverse layer is added to force the feature generator to produce domain invariant features.The feature generator is intially optimised with the self distillation and domain loss. The features are then detached and refed to domain discriminator to optimise domain discriminator with domain loss. This ensures that gradients of feature generator aren't changed after it is optimised)\n","al,lr1 variables should be replaced with the appropriate values"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T20:37:27.699006Z","iopub.status.busy":"2024-04-24T20:37:27.698613Z","iopub.status.idle":"2024-04-24T21:00:49.323165Z","shell.execute_reply":"2024-04-24T21:00:49.321815Z","shell.execute_reply.started":"2024-04-24T20:37:27.698974Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchaudio\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import optuna\n","from optuna.trial import TrialState\n","from torch.utils.data import random_split\n","from matplotlib.pylab import plt\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","BATCH_SIZE = 128\n","EPOCHS = 20\n","# LEARNING_RATE = 0.001\n","softmax = nn.Softmax(dim=1)\n","def create_data_loader(train_data, batch_size):\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size,shuffle=True,generator=g)\n","    return train_dataloader\n","\n","def kd_loss_function(output, target_output):\n","    temperature=3\n","    output = output / temperature\n","    output_log_softmax = torch.log_softmax(output, dim=1)\n","    loss_kd = -torch.mean(torch.sum(output_log_softmax * target_output, dim=1))\n","    return loss_kd\n","\n","\n","def train_single_epoch(trial,model, data_loader,val_loader, loss_fn,optimiser_f, device,i,alpha,train_losses,val_losses,optimiser_d,domain_classifier,domain_loss,h_score_tr,h_score_v,tr_acc,val_acc):\n","    temperature=3\n","    correct=0\n","    fet_a=[]\n","    label_a=[]\n","    domain_a=[]\n","    total=0\n","    y_pred_tr=[]\n","    y_true_tr=[]\n","    total_train_loss=0\n","    total_steps=0\n","    correct_domain=0\n","    total_domain=0\n","    model.train()\n","    domain_classifier.train()\n","    for inp, target,domain,triplet in data_loader:\n","        inp = inp.to(device)\n","        target=target.to(device)\n","        domain=domain.to(device)\n","        triplet=triplet.to(device)\n","        middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","        optimiser_f.zero_grad()\n","        fet_a.append(features)\n","        label_a.append(target)\n","        domain_a.append(domain)\n","        spoof_zero_mask = (target == 0).to(device)\n","        spoof_zero_inputs = features[spoof_zero_mask]\n","        spoof_zero_domains = domain[spoof_zero_mask]\n","        if spoof_zero_inputs.size(0) > 0:\n","            domain_output = domain_classifier(spoof_zero_inputs)\n","            d_loss=domain_loss(domain_output, spoof_zero_domains)\n","        temp4 = prediction / temperature\n","        temp4 = torch.softmax(temp4, dim=1)\n","        probabilities = softmax(prediction.detach())\n","        predicted_classes = torch.argmax(probabilities, dim=1)\n","        correct += (predicted_classes == target).sum().item()\n","        total += target.size(0)\n","        y_pred_tr.extend(predicted_classes.data.cpu().numpy())\n","        y_true_tr.extend(target.data.cpu().numpy())\n","        loss1by4 = kd_loss_function(middle_output1, temp4.detach())* (temperature**2)\n","\n","        loss2by4 = kd_loss_function(middle_output2, temp4.detach()) * (temperature**2)\n","\n","        loss3by4 = kd_loss_function(middle_output3, temp4.detach()) * (temperature**2)\n","    \n","        loss = loss_fn(prediction, target)\n","        total_loss = (1 - alpha) * (loss1by4 + loss2by4 + loss3by4) +  alpha *loss+d_loss\n","        total_train_loss+=total_loss.detach().cpu().numpy()\n","        total_steps+=1\n","\n","        total_loss.backward()\n","        optimiser_f.step()\n","        optimiser_d.zero_grad()\n","        feats=features.detach()\n","        dom=domain.detach()\n","        spoof_zero_inputs = feats[spoof_zero_mask]\n","        spoof_zero_domains = dom[spoof_zero_mask]\n","        if spoof_zero_inputs.size(0) > 0:\n","            domain_output = domain_classifier(spoof_zero_inputs)\n","            d_loss=domain_loss(domain_output, spoof_zero_domains)\n","            probabilities_domain=softmax(domain_output.detach())\n","            predicted_domains=torch.argmax(probabilities_domain, dim=1)\n","            correct_domain+=(predicted_domains == spoof_zero_domains).sum().item()\n","            total_domain+=spoof_zero_domains.size(0)\n","            d_loss.backward()\n","            optimiser_d.step()\n","    train_losses.append(total_train_loss/total_steps)\n","    print(\"train loss= {}\".format(total_train_loss/total_steps))\n","    accuracy = correct / total\n","    print(\"train   Accuracy = {}\".format(accuracy))\n","    domain_accuracy=correct_domain/total_domain\n","    print(\"train domain   Accuracy = {}\".format(domain_accuracy))\n","    tr_acc.append(accuracy)\n","\n","\n","    val_loss = 0.0\n","    val_steps = 0\n","    total_val = 0\n","    correct_val = 0\n","    y_pred_v=[]\n","    y_true_v=[]\n","    domain_pred_v=[]\n","    domain_true_v=[]\n","    fet_a_v=[]\n","    label_a_v=[]\n","    domain_a_v=[]\n","    correct_domain_val=0\n","    total_domain_val=0\n","    model.eval()\n","    domain_classifier.eval()\n","    for inp, target,domain,triplet in val_loader:\n","            with torch.no_grad():\n","                inp=inp.to(device)\n","                target=target.to(device)\n","                domain=domain.to(device)\n","                triplet=triplet.to(device)\n","                middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","                fet_a_v.append(features)\n","                label_a_v.append(target)\n","                domain_a_v.append(domain)\n","                spoof_zero_mask = (target == 0).to(device)\n","                spoof_zero_inputs = features[spoof_zero_mask]\n","                spoof_zero_domains = domain[spoof_zero_mask]\n","                if spoof_zero_inputs.size(0) > 0:\n","                    domain_output = domain_classifier(spoof_zero_inputs)\n","                    d_loss=domain_loss(domain_output,spoof_zero_domains)\n","                    probabilities_domain=softmax(domain_output.detach())\n","                    predicted_domains=torch.argmax(probabilities_domain, dim=1)\n","                    correct_domain_val+=(predicted_domains ==spoof_zero_domains).sum().item()\n","                    total_domain_val+=spoof_zero_domains.size(0)\n","                    domain_pred_v.extend(predicted_domains.data.cpu().numpy())\n","                    domain_true_v.extend(spoof_zero_domains.data.cpu().numpy())\n","                temp4 = prediction / temperature\n","                temp4 = torch.softmax(temp4, dim=1)\n","                probabilities = softmax(prediction)\n","                predicted_classes = torch.argmax(probabilities, dim=1)\n","                correct_val += (predicted_classes == target).sum().item()\n","                total_val += target.size(0)\n","                y_pred_v.extend(predicted_classes.data.cpu().numpy())\n","                y_true_v.extend(target.data.cpu().numpy())\n","                loss1by4 = kd_loss_function(middle_output1, temp4.detach())* (temperature**2)\n","                loss2by4 = kd_loss_function(middle_output2, temp4.detach()) * (temperature**2)\n","                loss3by4 = kd_loss_function(middle_output3, temp4.detach()) * (temperature**2)\n","                loss = loss_fn(prediction, target)\n","                total_loss_val = (1 - alpha) * (loss1by4 + loss2by4 + loss3by4) + alpha *loss +d_loss         \n","                val_loss += total_loss_val.detach().cpu().numpy()\n","                val_steps += 1\n","    val_accuracy = correct_val / total_val\n","    domain_acc_val=correct_domain_val/total_domain_val\n","    print(\"val loss = {}\".format(val_loss/val_steps))\n","    val_losses.append(val_loss/val_steps)\n","    print(\"val  Accuracy = {}\".format(val_accuracy))\n","    val_acc.append(val_accuracy)\n","    print(\"val  domain Accuracy = {}\".format(domain_acc_val))\n","    trial.report(domain_acc_val, i)\n","    if trial.should_prune():\n","        raise optuna.exceptions.TrialPruned()\n","    return val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a_v,label_a_v,domain_a_v,domain_acc_val,domain_pred_v,domain_true_v\n","\n","def train(trial):\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    else:\n","        device = \"cpu\"\n","    print(f\"Using {device}\")\n","    torch.manual_seed(0)\n","    epochs=20\n","    usd = UrbanSoundDataset(ANNOTATIONS_FILE,DOMAIN_PATH,AUDIO_DIR,\n","                            device)\n","    test_abs = int(len(usd) * 0.8)\n","    train_subset, val_subset = random_split(usd, [test_abs, len(usd) - test_abs],generator=torch.Generator().manual_seed(500))\n","    train_dataloader = create_data_loader(train_subset, BATCH_SIZE)\n","    val_loader=create_data_loader(val_subset,BATCH_SIZE)\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    domain_loss=torch.nn.CrossEntropyLoss()\n","    model = eca_resnet18().to(device)\n","    domain_model=domain_disc().to(device)\n","    lrd = trial.suggest_float(\"lrd\", 1e-5, 1e-1, log=True)\n","    optimiser = torch.optim.SGD(model.parameters(),lr=lr1)\n","    alpha= al\n","\n","    optimiser_domain=torch.optim.SGD(domain_model.parameters(),\n","                                 lr=lrd)\n","    train_losses=[]\n","    val_losses=[]\n","    h_score_v=[]\n","    h_score_tr=[]\n","    tr_acc=[]\n","    val_acc=[]\n","    for i in range(epochs):\n","        print(f\"Epoch {i+1}\")\n","        val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a,domain_acc_val,d_p,d_t=train_single_epoch(trial,model, train_dataloader,val_loader,loss_fn, optimiser, device,i,alpha,train_losses,val_losses,optimiser_domain,domain_model,domain_loss,h_score_tr,h_score_v,tr_acc,val_acc)\n","        print(\"---------------------------\")\n","    print(\"Finished training\")\n","    epochs_plot = range(1, 21)\n","    plt.plot(epochs_plot, train_losses, label='Training Loss')\n","    plt.plot(epochs_plot, val_losses, label='Validation Loss')\n","    classes = ('genuine','spoof')\n","    classes1=('D 0','D 1','D 2')\n","    cf_matrix1 = confusion_matrix(y_true_tr, y_pred_tr)\n","    cf_matrix2 = confusion_matrix(y_true_v, y_pred_v)\n","    cf_matrix3 = confusion_matrix(d_p, d_t)\n","    df_cm1 = pd.DataFrame(cf_matrix1 / np.sum(cf_matrix1, axis=1)[:, None], index = [i for i in classes],\n","                     columns = [i for i in classes])\n","    df_cm2 = pd.DataFrame(cf_matrix2 / np.sum(cf_matrix2, axis=1)[:, None], index = [i for i in classes],\n","                     columns = [i for i in classes])\n","    df_cm3 = pd.DataFrame(cf_matrix3 / np.sum(cf_matrix3, axis=1)[:, None], index = [i for i in classes1],\n","                     columns = [i for i in classes1])\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm1, annot=True)\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm2, annot=True)\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm3, annot=True)\n","    tsned(fet_a,label_a,domain_a)\n","    epochs_plot = range(1, 21)\n","    plt.plot(epochs_plot, tr_acc, label='Training Accuracy')\n","    plt.plot(epochs_plot, val_acc, label='Validation Accuracy')\n","    plt.legend()\n","    plt.show()\n","    return domain_acc_val\n","\n","\n","if __name__ == \"__main__\":\n","\n","    study = optuna.create_study(direction=\"maximize\")\n","    study.optimize(train, n_trials=10)\n","    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n","    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n","\n","    print(\"Study statistics: \")\n","    print(\"  Number of finished trials: \", len(study.trials))\n","    print(\"  Number of pruned trials: \", len(pruned_trials))\n","    print(\"  Number of complete trials: \", len(complete_trials))\n","\n","    print(\"Best trial:\")\n","    trial = study.best_trial\n","\n","    print(\"  Value: \", trial.value)\n","\n","    print(\"  Params: \")\n","    for key, value in trial.params.items():\n","        print(\"    {}: {}\".format(key, value))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import torch\n","# import torchaudio\n","# from torch import nn\n","# from torch.utils.data import DataLoader\n","# import numpy as np\n","# import optuna\n","# from optuna.trial import TrialState\n","# from torch.utils.data import random_split\n","# from matplotlib.pylab import plt\n","# from sklearn.metrics import confusion_matrix\n","# import seaborn as sn\n","# import pandas as pd\n","# BATCH_SIZE = 128\n","# EPOCHS = 20\n","# # LEARNING_RATE = 0.001\n","# softmax = nn.Softmax(dim=1)\n","# # rev=GradientReverse(1)\n","\n","# def create_data_loader(train_data, batch_size):\n","#     train_dataloader = DataLoader(train_data, batch_size=batch_size,shuffle=True)\n","#     return train_dataloader\n","\n","# def kd_loss_function(output, target_output):\n","#     temperature=3\n","#     output = output / temperature\n","#     output_log_softmax = torch.log_softmax(output, dim=1)\n","#     loss_kd = -torch.mean(torch.sum(output_log_softmax * target_output, dim=1))\n","#     return loss_kd\n","\n","\n","# def train_single_epoch(trial,model, data_loader,val_loader, loss_fn,optimiser_f, device,i,alpha,train_losses,val_losses,optimiser_d,domain_classifier,domain_loss):\n","#     temperature=3\n","#     correct=0\n","#     fet_a=[]\n","#     label_a=[]\n","#     domain_a=[]\n","#     total=0\n","#     y_pred_tr=[]\n","#     y_true_tr=[]\n","#     total_train_loss=0\n","#     total_steps=0\n","#     correct_domain=0\n","#     total_domain=0\n","#     model.train()\n","#     domain_classifier.train()\n","#     for inp, target,domain in data_loader:\n","#         inp = inp.to(device)\n","#         target=target.to(device)\n","#         domain=domain.to(device)\n","#         middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","#         optimiser_f.zero_grad()\n","#         spoof_zero_mask = (target == 0).to(device)\n","#         spoof_zero_inputs = features[spoof_zero_mask]\n","#         spoof_zero_domains = domain[spoof_zero_mask]\n","#         if spoof_zero_inputs.size(0) > 0:\n","#             domain_output = domain_classifier(spoof_zero_inputs)\n","#             d_loss=domain_loss(domain_output, spoof_zero_domains)\n","#         else:\n","#             d_loss=0\n","#         temp4 = prediction / temperature\n","#         temp4 = torch.softmax(temp4, dim=1)\n","#         probabilities = softmax(prediction.detach())\n","#         predicted_classes = torch.argmax(probabilities, dim=1)\n","#         correct += (predicted_classes == target).sum().item()\n","#         total += target.size(0)\n","#         y_pred_tr.extend(predicted_classes.data.cpu().numpy())\n","#         y_true_tr.extend(target.data.cpu().numpy())\n","#         loss1by4 = kd_loss_function(middle_output1, temp4.detach())* (temperature**2)\n","#         loss2by4 = kd_loss_function(middle_output2, temp4.detach()) * (temperature**2)\n","#         loss3by4 = kd_loss_function(middle_output3, temp4.detach()) * (temperature**2)\n","#         loss = loss_fn(prediction, target)\n","#         total_loss = (1 - alpha) * (loss1by4 + loss2by4 + loss3by4) +  alpha *loss +d_loss\n","#         total_train_loss+=total_loss.detach().cpu().numpy()\n","#         total_steps+=1\n","#         total_loss.backward()\n","#         optimiser_f.step()\n","#         optimiser_d.zero_grad()\n","#         spoof_zero_inputs = features.detach()[spoof_zero_mask]\n","#         spoof_zero_domains = domain.detach()[spoof_zero_mask]\n","#         if spoof_zero_inputs.size(0) > 0:\n","#             domain_output = domain_classifier(spoof_zero_inputs)\n","#             d_loss=domain_loss(domain_output, spoof_zero_domains)\n","#             probabilities_domain=softmax(domain_output.detach())\n","#             predicted_domains=torch.argmax(probabilities_domain, dim=1)\n","#             correct_domain+=(predicted_domains == spoof_zero_domains).sum().item()\n","#             total_domain+=spoof_zero_domains.size(0)\n","#             d_loss.backward()\n","#             optimiser_d.step()\n","#     train_losses.append(total_train_loss/total_steps)\n","#     tll=total_train_loss/total_steps\n","#     print(\"train loss = {}\".format(tll))\n","#     accuracy = correct / total\n","#     print(\"train   Accuracy = {}\".format(accuracy))\n","#     domain_accuracy=correct_domain/total_domain\n","#     print(\"train domain   Accuracy = {}\".format(domain_accuracy))\n","\n","#     val_loss = 0.0\n","#     val_steps = 0\n","#     total_val = 0\n","#     correct_val = 0\n","#     y_pred_v=[]\n","#     y_true_v=[]\n","#     fet_a=[]\n","#     label_a=[]\n","#     domain_a=[]\n","#     correct_domain_val=0\n","#     total_domain_val=0\n","#     model.eval()\n","#     domain_classifier.eval()\n","#     for inp, target,domain in val_loader:\n","#             with torch.no_grad():\n","#                 inp=inp.to(device)\n","#                 target=target.to(device)\n","#                 domain=domain.to(device)\n","#                 middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","#                 fet_a.append(features)\n","#                 label_a.append(target)\n","#                 domain_a.append(domain)\n","#                 spoof_zero_mask = (target == 0).to(device)\n","#                 spoof_zero_inputs = features[spoof_zero_mask]\n","#                 spoof_zero_domains = domain[spoof_zero_mask]\n","#                 if spoof_zero_inputs.size(0) > 0:\n","#                     domain_output = domain_classifier(spoof_zero_inputs)\n","#                     d_loss=domain_loss(domain_output,spoof_zero_domains)\n","#                     probabilities_domain=softmax(domain_output.detach())\n","#                     predicted_domains=torch.argmax(probabilities_domain, dim=1)\n","#                     correct_domain_val+=(predicted_domains ==spoof_zero_domains).sum().item()\n","#                     total_domain_val+=spoof_zero_domains.size(0)\n","#                 temp4 = prediction / temperature\n","#                 temp4 = torch.softmax(temp4, dim=1)\n","#                 probabilities = softmax(prediction)\n","#                 predicted_classes = torch.argmax(probabilities, dim=1)\n","#                 correct_val += (predicted_classes == target).sum().item()\n","#                 total_val += target.size(0)\n","#                 y_pred_v.extend(predicted_classes.data.cpu().numpy())\n","#                 y_true_v.extend(target.data.cpu().numpy())\n","#                 loss1by4 = kd_loss_function(middle_output1, temp4.detach())* (temperature**2)\n","#                 loss2by4 = kd_loss_function(middle_output2, temp4.detach()) * (temperature**2)\n","#                 loss3by4 = kd_loss_function(middle_output3, temp4.detach()) * (temperature**2)\n","#                 loss = loss_fn(prediction, target)\n","#                 total_loss_val = (1 - alpha) * (loss1by4 + loss2by4 + loss3by4) + alpha *loss  +d_loss         \n","#                 val_loss += total_loss_val.detach().cpu().numpy()\n","#                 val_steps += 1\n","#     val_accuracy = correct_val / total_val\n","#     domain_acc_val=correct_domain_val/total_domain_val\n","#     vll=val_loss/val_steps\n","#     print(\"val loss = {}\".format(vll))\n","#     val_losses.append(val_loss/val_steps)\n","#     print(\"val  Accuracy = {}\".format(val_accuracy))\n","#     print(\"val  domain Accuracy = {}\".format(domain_acc_val))\n","#     trial.report(domain_acc_val, i)\n","#     if trial.should_prune():\n","#         raise optuna.exceptions.TrialPruned()\n","#     return val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a,domain_acc_val\n","\n","# def train(trial):\n","#     if torch.cuda.is_available():\n","#         device = \"cuda\"\n","#     else:\n","#         device = \"cpu\"\n","#     print(f\"Using {device}\")\n","#     epochs=20\n","#     usd = UrbanSoundDataset(ANNOTATIONS_FILE,DOMAIN_PATH,AUDIO_DIR,\n","#                             device)\n","#     test_abs = int(len(usd) * 0.8)\n","#     train_subset, val_subset = random_split(usd, [test_abs, len(usd) - test_abs],generator=torch.Generator().manual_seed(500))\n","#     train_dataloader = create_data_loader(train_subset, BATCH_SIZE)\n","#     val_loader=create_data_loader(val_subset,BATCH_SIZE)\n","#     loss_fn = torch.nn.CrossEntropyLoss()\n","#     domain_loss=torch.nn.CrossEntropyLoss()\n","#     torch.manual_seed(0)\n","#     model = eca_resnet18().to(device)\n","#     domain_model=domain_disc().to(device)\n","#     lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n","#     alpha=al\n","#     optimiser = torch.optim.SGD(model.parameters(),\n","#                                  lr=lr1)\n","#     optmiser_domain=torch.optim.SGD(domain_model.parameters(),\n","#                                  lr=lr)\n","#     train_losses=[]\n","#     val_losses=[]\n","#     for i in range(epochs):\n","#         print(f\"Epoch {i+1}\")\n","#         val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a,domain_acc_val=train_single_epoch(trial,model, train_dataloader,val_loader,loss_fn, optimiser, device,i,alpha,train_losses,val_losses,optmiser_domain,domain_model,domain_loss)\n","#         print(\"---------------------------\")\n","#     print(\"Finished training\")\n","#     epochs_plot = range(1, 21)\n","#     plt.plot(epochs_plot, train_losses, label='Training Loss')\n","#     plt.plot(epochs_plot, val_losses, label='Validation Loss')\n","#     classes = ('genuine','spoof')\n","#     cf_matrix1 = confusion_matrix(y_true_tr, y_pred_tr)\n","#     cf_matrix2 = confusion_matrix(y_true_v, y_pred_v)\n","#     df_cm1 = pd.DataFrame(cf_matrix1 / np.sum(cf_matrix1, axis=1)[:, None], index = [i for i in classes],\n","#                      columns = [i for i in classes])\n","#     df_cm2 = pd.DataFrame(cf_matrix2 / np.sum(cf_matrix2, axis=1)[:, None], index = [i for i in classes],\n","#                      columns = [i for i in classes])\n","#     plt.figure(figsize = (12,7))\n","#     sn.heatmap(df_cm1, annot=True)\n","#     plt.figure(figsize = (12,7))\n","#     sn.heatmap(df_cm2, annot=True)\n","#     tsned(fet_a,label_a,domain_a)\n","#     return domain_acc_val\n","\n","\n","# if __name__ == \"__main__\":\n","\n","#     study = optuna.create_study(direction=\"maximize\")\n","#     study.optimize(train, n_trials=5)\n","#     pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n","#     complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n","\n","#     print(\"Study statistics: \")\n","#     print(\"  Number of finished trials: \", len(study.trials))\n","#     print(\"  Number of pruned trials: \", len(pruned_trials))\n","#     print(\"  Number of complete trials: \", len(complete_trials))\n","\n","#     print(\"Best trial:\")\n","#     trial = study.best_trial\n","\n","#     print(\"  Value: \", trial.value)\n","\n","#     print(\"  Params: \")\n","#     for key, value in trial.params.items():\n","#         print(\"    {}: {}\".format(key, value))\n"]},{"cell_type":"markdown","metadata":{},"source":["Triplet Mining Function is described below : We do online triplet mining and make use of semi hard triplet loss"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T21:38:07.681040Z","iopub.status.busy":"2024-04-25T21:38:07.680640Z","iopub.status.idle":"2024-04-25T21:38:07.701757Z","shell.execute_reply":"2024-04-25T21:38:07.700797Z","shell.execute_reply.started":"2024-04-25T21:38:07.681010Z"},"trusted":true},"outputs":[],"source":["import random\n","from itertools import combinations\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class OnlineTripleLoss(nn.Module):\n","    def __init__(self, margin, sampling_strategy=\"random_sh\"):\n","        super(OnlineTripleLoss, self).__init__()\n","        self.margin = margin\n","        self.triplet_selector = NegativeTripletSelector(\n","            margin, sampling_strategy\n","        )\n","\n","    def forward(self, embeddings, labels):\n","        triplets = self.triplet_selector.get_triplets(embeddings, labels)\n","        ap_dists = F.pairwise_distance(\n","            embeddings[triplets[0], :], embeddings[triplets[1], :]\n","        )\n","        an_dists = F.pairwise_distance(\n","            embeddings[triplets[0], :], embeddings[triplets[2], :]\n","        )\n","        loss = F.relu(ap_dists - an_dists + self.margin)\n","        return loss.mean(), len(triplets[0])\n","\n","\n","class NegativeTripletSelector:\n","    def __init__(self, margin, sampling_strategy=\"random_sh\"):\n","        super(NegativeTripletSelector, self).__init__()\n","        self.margin = margin\n","        self.sampling_strategy = sampling_strategy\n","\n","    def get_triplets(self, embeddings, labels):\n","        distance_matrix = pdist(embeddings, eps=0)\n","        unique_labels, counts = torch.unique(labels, return_counts=True)\n","        triplets_indices = [[] for i in range(3)]\n","        for i, label in enumerate(unique_labels):\n","            label_mask = labels == label\n","            label_indices = torch.where(label_mask)[0]\n","            if label_indices.shape[0] < 2:\n","                continue\n","            negative_indices = torch.where(torch.logical_not(label_mask))[0]\n","            triplet_label_pairs = self.get_one_one_triplets(\n","                label_indices, negative_indices, distance_matrix,\n","            )\n","\n","            triplets_indices[0].extend(triplet_label_pairs[0])\n","            triplets_indices[1].extend(triplet_label_pairs[1])\n","            triplets_indices[2].extend(triplet_label_pairs[2])\n","\n","        return triplets_indices\n","\n","    def get_one_one_triplets(self, pos_labels, negative_indices, dist_mat):\n","        anchor_positives = list(combinations(pos_labels, 2))\n","        triplets_indices = [[] for i in range(3)]\n","        for i, anchor_positive in enumerate(anchor_positives):\n","            anchor_idx = anchor_positive[0]\n","            pos_idx = anchor_positive[1]\n","            ap_dist = dist_mat[anchor_idx, pos_idx]\n","            an_dists = dist_mat[anchor_idx, negative_indices]\n","            if self.sampling_strategy == \"random_sh\":\n","                neg_list_idx = random_semi_hard_sampling(\n","                    ap_dist, an_dists, self.margin\n","                )\n","            elif self.sampling_strategy == \"fixed_sh\":\n","                neg_list_idx = fixed_semi_hard_sampling(\n","                    ap_dist, an_dists, self.margin\n","                )\n","            else:\n","                neg_list_idx = None\n","            if neg_list_idx is not None:\n","                neg_idx = negative_indices[neg_list_idx]\n","                triplets_indices[0].append(anchor_idx)\n","                triplets_indices[1].append(pos_idx)\n","                triplets_indices[2].append(neg_idx)\n","        return triplets_indices\n","\n","\n","def random_semi_hard_sampling(ap_dist, an_dists, margin):\n","    ap_margin_dist = ap_dist + margin\n","    loss = ap_margin_dist - an_dists\n","    possible_negs = torch.where(loss > 0)[0]\n","    if possible_negs.nelement() != 0:\n","        neg_idx = random.choice(possible_negs)\n","    else:\n","        neg_idx = None\n","    return neg_idx\n","\n","\n","def fixed_semi_hard_sampling(ap_dist, an_dists, margin):\n","    ap_margin_dist = ap_dist + margin\n","    loss = ap_margin_dist - an_dists\n","    possible_negs = torch.where(loss > 0)[0]\n","    if possible_negs.nelement() != 0:\n","        neg_idx = torch.argmax(loss).item()\n","    else:\n","        neg_idx = None\n","    # neg_idx = torch.argmin(an_dists).item()\n","    return neg_idx\n","\n","\n","def pdist(vectors, eps):\n","    dist_mat = []\n","    for i in range(len(vectors)):\n","        dist_mat.append(\n","            F.pairwise_distance(vectors[i], vectors, eps=eps).unsqueeze(0)\n","        )\n","    return torch.cat(dist_mat, dim=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torchaudio\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import optuna\n","from optuna.trial import TrialState\n","from torch.utils.data import random_split\n","from matplotlib.pylab import plt\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","BATCH_SIZE = 64\n","EPOCHS = 20\n","# LEARNING_RATE = 0.001\n","softmax = nn.Softmax(dim=1)\n","\n","def create_data_loader(train_data, batch_size):\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size,shuffle=True)\n","    return train_dataloader\n","\n","def kd_loss_function(output, target_output):\n","    temperature=3\n","    output = output / temperature\n","    output_log_softmax = torch.log_softmax(output, dim=1)\n","    loss_kd = -torch.mean(torch.sum(output_log_softmax * target_output, dim=1))\n","    return loss_kd\n","\n","\n","def train_single_epoch(model, data_loader,val_loader, loss_fn,optimiser_f, device,i,alpha,train_losses,val_losses,optimiser_d,domain_classifier,domain_loss, criterion_triplet):\n","    temperature=3\n","    correct=0\n","    fet_a=[]\n","    label_a=[]\n","    domain_a=[]\n","    total=0\n","    y_pred_tr=[]\n","    y_true_tr=[]\n","    y_one_tr=[]\n","    total_train_loss=0\n","    total_steps=0\n","    correct_domain=0\n","    total_domain=0\n","    model.train()\n","    domain_classifier.train()\n","    for inp, target,domain,tr_label in data_loader:\n","        inp = inp.to(device)\n","        target=target.to(device)\n","        domain=domain.to(device)\n","        tr_label=tr_label.to(device)\n","        middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","        optimiser_f.zero_grad()\n","        temp4 = prediction / temperature\n","        temp4 = torch.softmax(temp4, dim=1)\n","        probabilities = softmax(prediction.detach())\n","        predicted_classes = torch.argmax(probabilities, dim=1)\n","        correct += (predicted_classes == target).sum().item()\n","        total += target.size(0)\n","        label_1_probabilities = probabilities[:, 1]\n","        y_pred_tr.extend(predicted_classes.data.cpu().numpy())\n","        y_one_tr.extend(label_1_probabilities.data.cpu().numpy())\n","        y_true_tr.extend(target.data.cpu().numpy())\n","        triplet_loss,_=criterion_triplet(tr_label,features)\n","        loss1by4 = kd_loss_function(middle_output1, temp4.detach())* (temperature**2)\n","\n","\n","        loss2by4 = kd_loss_function(middle_output2, temp4.detach()) * (temperature**2)\n","\n","\n","        loss3by4 = kd_loss_function(middle_output3, temp4.detach()) * (temperature**2)\n","        loss = loss_fn(prediction, target)\n","        total_loss = (1 - alpha) * (loss1by4 + loss2by4 + loss3by4) +  alpha *loss + triplet_loss\n","        total_train_loss+=total_loss.detach().cpu().numpy()\n","        total_steps+=1\n","\n","        total_loss.backward()\n","        optimiser_f.step()\n","    train_losses.append(total_train_loss/total_steps)\n","    print(\"train  loss= {}\".format(total_train_loss/total_steps))\n","    eer_t=compute_eer(y_true_tr,y_one_tr)\n","    print(\"train eer = {}\".format(eer_t))\n","    accuracy = correct / total\n","    print(\"train   Accuracy = {}\".format(accuracy))\n","\n","    val_loss = 0.0\n","    val_steps = 0\n","    total_val = 0\n","    correct_val = 0\n","    y_pred_v=[]\n","    y_true_v=[]\n","    domain_pred_v=[]\n","    domain_true_v=[]\n","    fet_a=[]\n","    label_a=[]\n","    domain_a=[]\n","    y_one_v=[]\n","    correct_domain_val=0\n","    total_domain_val=0\n","    model.eval()\n","    domain_classifier.eval()\n","    for inp, target,domain in val_loader:\n","            with torch.no_grad():\n","                inp=inp.to(device)\n","                target=target.to(device)\n","                domain=domain.to(device)\n","#                 tr_label=tr_label.to(device)\n","                middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","                fet_a.append(features)\n","                label_a.append(target)\n","                domain_a.append(domain)\n","                temp4 = prediction / temperature\n","                temp4 = torch.softmax(temp4, dim=1)\n","                probabilities = softmax(prediction)\n","                predicted_classes = torch.argmax(probabilities, dim=1)\n","                correct_val += (predicted_classes == target).sum().item()\n","                total_val += target.size(0)\n","                label_1_probabilities = probabilities[:, 1]\n","                y_pred_v.extend(predicted_classes.data.cpu().numpy())\n","                y_one_v.extend(label_1_probabilities.data.cpu().numpy())\n","                y_true_v.extend(target.data.cpu().numpy())\n","                triplet_loss,_=criterion_triplet(features,tr_label)\n","                loss1by4 = kd_loss_function(middle_output1, temp4.detach())* (temperature**2)\n","                loss2by4 = kd_loss_function(middle_output2, temp4.detach()) * (temperature**2)\n","                loss3by4 = kd_loss_function(middle_output3, temp4.detach()) * (temperature**2)\n","                loss = loss_fn(prediction, target)\n","                total_loss_val = (1 - alpha) * (loss1by4 + loss2by4 + loss3by4) + alpha *loss +triplet_loss        \n","                val_loss += total_loss_val.detach().cpu().numpy()\n","                val_steps += 1\n","    val_accuracy = correct_val / total_val\n","    print(\"val  Accuracy = {}\".format(val_loss/val_steps))\n","    val_losses.append(val_loss/val_steps)\n","    print(\"val  Accuracy = {}\".format(val_accuracy))\n","    eer_v=compute_eer(y_true_v,y_one_v)\n","    print(\"val eer = {}\".format(eer_v))\n","    return val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a,y_one_tr,y_one_v\n","\n","def train():\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    else:\n","        device = \"cpu\"\n","    print(f\"Using {device}\")\n","    torch.manual_seed(0)\n","    epochs=20\n","    usd = UrbanSoundDataset(ANNOTATIONS_FILE,DOMAIN_PATH,AUDIO_DIR,\n","                            device)\n","    test_abs = int(len(usd) * 0.8)\n","    train_subset, val_subset = random_split(usd, [test_abs, len(usd) - test_abs],generator=torch.Generator().manual_seed(500))\n","    train_dataloader = create_data_loader(train_subset, BATCH_SIZE)\n","    val_loader=create_data_loader(val_subset,BATCH_SIZE)\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    domain_loss=torch.nn.CrossEntropyLoss()\n","    criterion_triplet = OnlineTripleLoss(\n","            margin=0.1,\n","            sampling_strategy=\"random_sh\"\n","        )\n","    model = eca_resnet18().to(device)\n","    domain_model=domain_disc().to(device)\n","    alpha=al\n","    optimiser = torch.optim.SGD(model.parameters(),\n","                                 lr=lr1)\n","    optmiser_domain=torch.optim.SGD(domain_model.parameters(),\n","                                 lr=lrd)\n","    train_losses=[]\n","    val_losses=[]\n","    for i in range(epochs):\n","        print(f\"Epoch {i+1}\")\n","        val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a,y_one_tr,y_one_v=train_single_epoch(model, train_dataloader,val_loader,loss_fn, optimiser, device,i,alpha,train_losses,val_losses,optmiser_domain,domain_model,domain_loss,criterion_triplet)\n","        print(\"---------------------------\")\n","    print(\"Finished training\")\n","    epochs_plot = range(1, 21)\n","    plt.plot(epochs_plot, train_losses, label='Training Loss')\n","    plt.plot(epochs_plot, val_losses, label='Validation Loss')\n","    classes = ('genuine','spoof')\n","#     classes1=('D 0','D 1','D 2')\n","    cf_matrix1 = confusion_matrix(y_true_tr, y_pred_tr)\n","    cf_matrix2 = confusion_matrix(y_true_v, y_pred_v)\n","    df_cm1 = pd.DataFrame(cf_matrix1 / np.sum(cf_matrix1, axis=1)[:, None], index = [i for i in classes],\n","                     columns = [i for i in classes])\n","    df_cm2 = pd.DataFrame(cf_matrix2 / np.sum(cf_matrix2, axis=1)[:, None], index = [i for i in classes],\n","                     columns = [i for i in classes])\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm1, annot=True)\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm2, annot=True)\n","    tsned(fet_a,label_a,domain_a)\n","    plot_det(y_true_tr, y_one_tr)\n","    plot_det(y_true_v, y_one_v)\n","    return model\n","\n","\n","if __name__ == \"__main__\":\n","\n","    model=train()\n","    torch.save(model.state_dict(), \"model_triplet.pth\")"]},{"cell_type":"markdown","metadata":{},"source":["Complete Proposed System\n","Self distillation , domain adversarial learning and triplet mining combined. Outputs training and validation loss, accuracy,confusion matrices and det curves, t-SNE visualization of validation set, NMI vs training accuracy over epochs, domain discriminator accuracy over epochs\n","al,lrd and lr1 replaced with appropriate parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T21:42:46.149077Z","iopub.status.busy":"2024-04-25T21:42:46.148733Z","iopub.status.idle":"2024-04-25T22:09:32.012832Z","shell.execute_reply":"2024-04-25T22:09:32.011895Z","shell.execute_reply.started":"2024-04-25T21:42:46.149053Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchaudio\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import optuna\n","from optuna.trial import TrialState\n","from torch.utils.data import random_split\n","from matplotlib.pylab import plt\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","BATCH_SIZE = 64\n","EPOCHS = 20\n","torch.manual_seed(0)\n","# LEARNING_RATE = 0.001\n","softmax = nn.Softmax(dim=1)\n","\n","def create_data_loader(train_data, batch_size):\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size,shuffle=True)\n","    return train_dataloader\n","\n","def kd_loss_function(output, target_output):\n","    temperature=3\n","    output = output / temperature\n","    output_log_softmax = torch.log_softmax(output, dim=1)\n","    loss_kd = -torch.mean(torch.sum(output_log_softmax * target_output, dim=1))\n","    return loss_kd\n","\n","\n","def train_single_epoch(model, data_loader,val_loader, loss_fn,optimiser_f, device,i,alpha,train_losses,val_losses,optimiser_d,domain_classifier,domain_loss,h_score_tr,h_score_v,tr_acc,val_acc,dom_tr,dom_v,criterion_triplet):\n","    temperature=3\n","    correct=0\n","    fet_a=[]\n","    label_a=[]\n","    domain_a=[]\n","    triplet_a=[]\n","    total=0\n","    y_pred_tr=[]\n","    y_true_tr=[]\n","    y_one_tr=[]\n","    y_one_v=[]\n","    total_train_loss=0\n","    total_steps=0\n","    correct_domain=0\n","    total_domain=0\n","    model.train()\n","    domain_classifier.train()\n","    for inp, target,domain,triplet in data_loader:\n","        inp = inp.to(device)\n","        target=target.to(device)\n","        domain=domain.to(device)\n","        triplet=triplet.to(device)\n","        middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","        optimiser_f.zero_grad()\n","        fet_a.append(features)\n","        label_a.append(target)\n","        domain_a.append(domain)\n","        triplet_a.append(triplet)\n","        spoof_zero_mask = (target == 0).to(device)\n","        spoof_zero_inputs = features[spoof_zero_mask]\n","        spoof_zero_domains = domain[spoof_zero_mask]\n","        if spoof_zero_inputs.size(0) > 0:\n","            domain_output = domain_classifier(spoof_zero_inputs)\n","            d_loss=domain_loss(domain_output, spoof_zero_domains)\n","        temp4 = prediction / temperature\n","        temp4 = torch.softmax(temp4, dim=1)\n","        probabilities = softmax(prediction.detach())\n","        predicted_classes = torch.argmax(probabilities, dim=1)\n","        correct += (predicted_classes == target).sum().item()\n","        total += target.size(0)\n","        y_pred_tr.extend(predicted_classes.data.cpu().numpy())\n","        y_true_tr.extend(target.data.cpu().numpy())\n","        label_1_probabilities = probabilities[:, 1]\n","        y_one_tr.extend(label_1_probabilities.data.cpu().numpy())\n","        loss1by4 = kd_loss_function(middle_output1, temp4.detach())* (temperature**2)\n","\n","        loss2by4 = kd_loss_function(middle_output2, temp4.detach()) * (temperature**2)\n","\n","        loss3by4 = kd_loss_function(middle_output3, temp4.detach()) * (temperature**2)\n","    \n","        loss = loss_fn(prediction, target)\n","        triplet_loss,_=criterion_triplet(features,triplet)\n","        total_loss = (1 - alpha) * (loss1by4 + loss2by4 + loss3by4) +  alpha *loss+d_loss+triplet_loss\n","        total_train_loss+=total_loss.detach().cpu().numpy()\n","        total_steps+=1\n","\n","        total_loss.backward()\n","        optimiser_f.step()\n","        optimiser_d.zero_grad()\n","        feats=features.detach()\n","        dom=domain.detach()\n","        spoof_zero_inputs = feats[spoof_zero_mask]\n","        spoof_zero_domains = dom[spoof_zero_mask]\n","        if spoof_zero_inputs.size(0) > 0:\n","            domain_output = domain_classifier(spoof_zero_inputs)\n","            d_loss=domain_loss(domain_output, spoof_zero_domains)\n","            probabilities_domain=softmax(domain_output.detach())\n","            predicted_domains=torch.argmax(probabilities_domain, dim=1)\n","            correct_domain+=(predicted_domains == spoof_zero_domains).sum().item()\n","            total_domain+=spoof_zero_domains.size(0)\n","            d_loss.backward()\n","            optimiser_d.step()\n","    train_losses.append(total_train_loss/total_steps)\n","    print(\"train loss = {}\".format(total_train_loss/total_steps))\n","    accuracy = correct / total\n","    print(\"train   Accuracy = {}\".format(accuracy))\n","    domain_accuracy=correct_domain/total_domain\n","    print(\"train domain   Accuracy = {}\".format(domain_accuracy))\n","    h_score=cluster_triplet(fet_a,label_a,triplet_a)\n","    h_score_tr.append(h_score)\n","    tr_acc.append(accuracy)\n","    eer=compute_eer(y_true_tr,y_one_tr)\n","    print(\"train eer = {}\".format(eer))\n","    dom_tr.append(domain_accuracy)\n","\n","\n","    val_loss = 0.0\n","    val_steps = 0\n","    total_val = 0\n","    correct_val = 0\n","    y_pred_v=[]\n","    y_true_v=[]\n","    domain_pred_v=[]\n","    domain_true_v=[]\n","    fet_a_v=[]\n","    label_a_v=[]\n","    domain_a_v=[]\n","    triplet_a_v=[]\n","    correct_domain_val=0\n","    total_domain_val=0\n","    model.eval()\n","    domain_classifier.eval()\n","    for inp, target,domain,triplet in val_loader:\n","            with torch.no_grad():\n","                inp=inp.to(device)\n","                target=target.to(device)\n","                domain=domain.to(device)\n","                triplet=triplet.to(device)\n","                middle_output1,middle_output2,middle_output3,prediction,features = model(inp)\n","                fet_a_v.append(features)\n","                label_a_v.append(target)\n","                domain_a_v.append(domain)\n","                triplet_a_v.append(triplet)\n","                spoof_zero_mask = (target == 0).to(device)\n","                spoof_zero_inputs = features[spoof_zero_mask]\n","                spoof_zero_domains = domain[spoof_zero_mask]\n","                if spoof_zero_inputs.size(0) > 0:\n","                    domain_output = domain_classifier(spoof_zero_inputs)\n","                    d_loss=domain_loss(domain_output,spoof_zero_domains)\n","                    probabilities_domain=softmax(domain_output.detach())\n","                    predicted_domains=torch.argmax(probabilities_domain, dim=1)\n","                    correct_domain_val+=(predicted_domains ==spoof_zero_domains).sum().item()\n","                    total_domain_val+=spoof_zero_domains.size(0)\n","                    domain_pred_v.extend(predicted_domains.data.cpu().numpy())\n","                    domain_true_v.extend(spoof_zero_domains.data.cpu().numpy())\n","                temp4 = prediction / temperature\n","                temp4 = torch.softmax(temp4, dim=1)\n","                probabilities = softmax(prediction)\n","                predicted_classes = torch.argmax(probabilities, dim=1)\n","                correct_val += (predicted_classes == target).sum().item()\n","                total_val += target.size(0)\n","                y_pred_v.extend(predicted_classes.data.cpu().numpy())\n","                y_true_v.extend(target.data.cpu().numpy())\n","                loss1by4 = kd_loss_function(middle_output1, temp4.detach())* (temperature**2)\n","                loss2by4 = kd_loss_function(middle_output2, temp4.detach()) * (temperature**2)\n","                loss3by4 = kd_loss_function(middle_output3, temp4.detach()) * (temperature**2)\n","                loss = loss_fn(prediction, target)\n","                triplet_loss,_=criterion_triplet(features,triplet)\n","                total_loss_val = (1 - alpha) * (loss1by4 + loss2by4 + loss3by4) + alpha *loss+triplet_loss+d_loss \n","                val_loss += total_loss_val.detach().cpu().numpy()\n","                val_steps += 1\n","                label_1_probabilities = probabilities[:, 1]\n","                y_one_v.extend(label_1_probabilities.data.cpu().numpy())\n","    val_accuracy = correct_val / total_val\n","    domain_acc_val=correct_domain_val/total_domain_val\n","    print(\"val  loss = {}\".format(val_loss/val_steps))\n","    val_losses.append(val_loss/val_steps)\n","    print(\"val  Accuracy = {}\".format(val_accuracy))\n","#     h_score=cluster_triplet(fet_a_v,label_a_v,triplet_a_v)\n","#     h_score_v.append(h_score)\n","    val_acc.append(val_accuracy)\n","    print(\"val  domain Accuracy = {}\".format(domain_acc_val))\n","    eer=compute_eer(y_true_v,y_one_v)\n","    print(\"val eer = {}\".format(eer))\n","    dom_v.append(domain_acc_val)\n","    return val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a_v,label_a_v,domain_a_v,domain_acc_val,domain_pred_v,domain_true_v,y_one_tr,y_one_v\n","\n","def train():\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    else:\n","        device = \"cpu\"\n","    print(f\"Using {device}\")\n","    epochs=20\n","    usd = UrbanSoundDataset(ANNOTATIONS_FILE,DOMAIN_PATH,AUDIO_DIR,\n","                            device)\n","    test_abs = int(len(usd) * 0.8)\n","    train_subset, val_subset = random_split(usd, [test_abs, len(usd) - test_abs])\n","    train_dataloader = create_data_loader(train_subset, BATCH_SIZE)\n","    val_loader=create_data_loader(val_subset,BATCH_SIZE)\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    domain_loss=torch.nn.CrossEntropyLoss()\n","    model = eca_resnet18().to(device)\n","    domain_model=domain_disc().to(device)\n","    alpha= al\n","    optimiser = torch.optim.SGD(model.parameters(),\n","                                 lr= lr1)\n","    optmiser_domain=torch.optim.SGD(domain_model.parameters(),\n","                                 lr=lrd)\n","    criterion_triplet = OnlineTripleLoss(\n","            margin=0.1,\n","            sampling_strategy=\"random_sh\"\n","        )\n","\n","    train_losses=[]\n","    val_losses=[]\n","    h_score_v=[]\n","    h_score_tr=[]\n","    tr_acc=[]\n","    val_acc=[]\n","    dom_tr=[]\n","    dom_v=[]\n","    for i in range(epochs):\n","        print(f\"Epoch {i+1}\")\n","        val_accuracy,y_pred_tr,y_true_tr,y_pred_v,y_true_v,fet_a,label_a,domain_a,domain_acc_val,d_p,d_t,y_one_tr,y_one_v=train_single_epoch(model, train_dataloader,val_loader,loss_fn, optimiser, device,i,alpha,train_losses,val_losses,optmiser_domain,domain_model,domain_loss,h_score_tr,h_score_v,tr_acc,val_acc,dom_tr,dom_v,criterion_triplet)\n","        print(\"---------------------------\")\n","    print(\"Finished training\")\n","    epochs_plot = range(1, 21)\n","    plt.title('Training and Validation Loss over Epochs')\n","    plt.plot(epochs_plot, train_losses, label='Training Loss')\n","    plt.plot(epochs_plot, val_losses, label='Validation Loss')\n","    classes = ('genuine','spoof')\n","    classes1=('D 0','D 1','D 2')\n","    cf_matrix1 = confusion_matrix(y_true_tr, y_pred_tr)\n","    cf_matrix2 = confusion_matrix(y_true_v, y_pred_v)\n","    cf_matrix3 = confusion_matrix(d_p, d_t)\n","    df_cm1 = pd.DataFrame(cf_matrix1 / np.sum(cf_matrix1, axis=1)[:, None], index = [i for i in classes],\n","                     columns = [i for i in classes])\n","    df_cm2 = pd.DataFrame(cf_matrix2 / np.sum(cf_matrix2, axis=1)[:, None], index = [i for i in classes],\n","                     columns = [i for i in classes])\n","    df_cm3 = pd.DataFrame(cf_matrix3 / np.sum(cf_matrix3, axis=1)[:, None], index = [i for i in classes1],\n","                     columns = [i for i in classes1])\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm1, annot=True)\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm2, annot=True)\n","    plt.figure(figsize = (12,7))\n","    sn.heatmap(df_cm3, annot=True)\n","    tsned(fet_a,label_a,domain_a)\n","    epochs_plot = range(1, 21)\n","    plt.title('NMI and Accuracy Across Epochs')\n","    plt.plot(epochs_plot, h_score_tr, label='Training NMI')\n","    plt.plot(epochs_plot, tr_acc, label='Training Accuracy')\n","    plt.legend()\n","    plt.show()\n","    plt.title('Training and Validation Accuracy over Epochs')\n","    plt.plot(epochs_plot, tr_acc, label='Training Accuracy')\n","    plt.plot(epochs_plot, val_acc, label='Validation Accuracy')\n","    plt.legend()\n","    plt.show()\n","    plt.title('Training Accuracy of Domain Discriminator over Epochs')\n","    plt.plot(epochs_plot, dom_tr, label='Training Accuracy')\n","    plt.legend()\n","    plt.show()\n","    plot_det(y_true_tr,y_one_tr)\n","    plot_det(y_true_v,y_one_v)\n","    return model\n","\n","\n","if __name__ == \"__main__\":\n","\n","    model_ev=train()\n","    torch.save(model_ev.state_dict(), \"model_complete.pth\")\n","    print(\"Trained feed forward net saved at feedforwardnet.pth\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4778682,"sourceId":8093857,"sourceType":"datasetVersion"},{"datasetId":4796858,"sourceId":8118486,"sourceType":"datasetVersion"},{"datasetId":4798616,"sourceId":8121068,"sourceType":"datasetVersion"},{"datasetId":4812049,"sourceId":8139353,"sourceType":"datasetVersion"},{"datasetId":4817728,"sourceId":8146870,"sourceType":"datasetVersion"},{"datasetId":4820345,"sourceId":8150465,"sourceType":"datasetVersion"},{"datasetId":4820790,"sourceId":8151086,"sourceType":"datasetVersion"},{"datasetId":4821883,"sourceId":8152569,"sourceType":"datasetVersion"},{"datasetId":4846037,"sourceId":8184382,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
